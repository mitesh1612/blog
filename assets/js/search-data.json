{
  
    
        "post0": {
            "title": "Improvements and Implementations of the Singleton Pattern",
            "content": "Singleton is probably one of the easiest design patterns from the Gang of Four book. Its easy to understand and fairly simple to implement. Then Why is it hated? How is it easy to misuse? . Let‚Äôs see step by step on what‚Äôs wrong with the pattern, how we can fix its flaws and collect some learnings on the way. . For the code examples, I will assume that we will be writing a service called SomeService as a singleton that can be used anywhere across a .NET app. This service was designed as a singleton and not passed using Dependency Injection to make it available as an ambient service across the code base, deep inside the business logic. The pain points still apply for using Singleton for any other class as well. . Just a heads-up, the code examples will be in C#, but the flaws and improvements work in whatever language you work with. üòÄ . Implementing Singleton using a Static Property . So the base, vanilla implementation of the singleton pattern is quite easy, you make the constructor private (or protected if that‚Äôs your jam) and provide a static instance property which will be used to access the singleton object. Here‚Äôs how that is implemented . public class SomeService { private SomeService() { Console.WriteLine(&quot;Some heavy service initialization&quot;); } private static readonly SomeService _instance = new SomeService(); public static SomeService Instance =&gt; _instance; // Other properties of this service } . And it can be used as . var someSvc = SomeService.Instance; . As I said, simple enough. This is the easiest implementation to work with. . Not, lets discuss a few cons of going with this default way. . Cons of Default Implementation . Not lazy. The instance will be instantiated when the type is loaded. This means whether the instance is used or not, it will always be loaded, and will cause a performance issue if the service initialization is really heavy. But that has an easy fix, more details below. | The singleton object cannot be replaced with the another type of object at runtime in a polymorphic way. Since there is no interface for the SomeService, I can only work with this one implementation of it. | The code that uses Singleton is not easily testable. Let‚Äôs say I have some code consuming this service. Since the code is relying on a static singleton, it‚Äôs actually hard to mock or stub it and that reduces the testability. Also since this is a static, it is generally bad for testing as discussed here. | Using a singleton creates implicit dependencies in the rest of the code. | It also violates the single responsibility principle because the class has 2 responsibilities, its main work and it is also responsible for creating itself. | . Let‚Äôs try to fix a few of these cons by making slight changes to the default implementation of the Singleton pattern. . Lazy implementation using Lazy&lt;T&gt; class . We can fix the first con by using Lazy&lt;T&gt; class. The singleton object will only be instantiated when the Instance property is actually accessed. . public class SomeService { private SomeService() { Console.WriteLine(&quot;Some heavy service initialization&quot;); } private static readonly Lazy&lt;SomeService&gt; _instance = new Lazy&lt;SomeService&gt;(() =&gt; new Singleton()); public static SomeService Instance =&gt; _instance.Value; // Other properties of this service } . This makes the implementation a 100% lazy and still maintains the ease of implementation. But the other cons still hold true here like no extensibility, it is not testable and it still violates the Single Responsibility Principle. . Double-Checked Locking based implementation . Yes, we are still editing the vanilla pattern a bit more. This implementation is almost similar to the previous ones, but from a learning standpoint, it provides a few standouts. It shows how to use doubled-checked locking technique, the volatile keyword and lock synchronization. This is a great implementation for a concurrent distributed system, since its completely thread safe. . public class SomeService { private static volatile SomeService _instance; private static readonly object _lock = new object(); private SomeService() { Console.WriteLine(&quot;Some heavy service initialization&quot;); } public static SomeService Instance { get { if (_instance == null) { lock (_lock) { if (_instance == null) { _instance = new SomeService(); } } } return _instance; } } } . Using this locking construct makes this singleton implementation thread safe. . This still doesn‚Äôt solve the other cons listed above and it also makes the implementation a bit complicated. . Singleton or Dependency Injection? . The main complaint for a default Singleton pattern is that is doesn‚Äôt support dependency injection principle which hinders testability for the code that consumes this singleton service. It acts as a global constant and its really hard to substitute it with a test double. . Dependency Injection is preferable when we have a non-stable dependency. It‚Äôs a good practice, as it provides great testability and lets a class specify everything it needs to function properly. . But then there are some dependencies that can be better represented using a Singleton. These are ambient dependencies. Ambient dependencies are dependencies which span across multiple classes and often multiple layers. They act as cross-cutting concerns for your application. . It doesn‚Äôt really make sense to apply the Dependency Injection design pattern for such dependencies as they are going to be everywhere anyway. . For such dependencies, its easier to make it ambient and clean up the dependency graph for consuming code. Without doing this, the code base would be flooded with the excessive number of the same dependencies traversing most of your classes. There might also be cases where you might need to create a tree of passing the dependency to some business logic class from the point where all dependencies are received. . Implementing Singleton using the Ambient Context Pattern . So one of the major flexibility for the default Singleton pattern is the lack of flexibility at runtime. Since any code consuming this service depends on the concrete implementation itself, we are unable to replace it with say an ExtendedSomeService or MockSomeService. To achieve extensiblity at runtime, while also having a global access point, we can use the Ambient Context Pattern. Let‚Äôs start with seeing the implementation directly. . (Also hey, sound off in the comments if you want to see my take on ambient service/context pattern). . public interface ISomeService { } public class SomeService : ISomeService { } public class MockSomeService : ISomeService { } public class SomeServiceContext { public static ISomeService Current { get; private set; } public static void Initialize(ISomeService object) { Current = object; } } . And then we can initialize or use the like so: . // Initialization in the application code SomeServiceContext.Initialize(new SomeService()); // Intialization in unit tests SomeServiceContext.Initialize(new MockSomeService()); . Now to access SomeService, we always use the SomeServiceContext.Current property and use the Initialize method to change the concrete implementation at runtime, thus giving us more flexibility. Also introducing the interface for SomeService allows us to extend or mock the existing service code and still make it usable with our SomeServiceContextClass to access the modified version without modiying the usage points of this service. . The most obvious pro of jumping through these hoops is to achieve runtime flexibility. Also since now SomeService only does its own work, we are not violating the Single Responsiblity Principle. . Now about the glaring Con: This is not the Singleton pattern anymore. As it is visibile clearly, now someone can create many many instances of the singleton service class and they might not be the same as accessed from the context class. . Another variant of this pattern using an initializer/factory method . Another variant of this same pattern is to use a factory method that is responsible to create the current instance, and it will look something like this: . public class SomeServiceContext { public static ISomeService Current { get; private set; } public static void Initialize(Func&lt;ISomeService&gt; someSvcFactory) { Current = someSvcFactory(); } } . It can also be implemented as: . public class SomeServiceContext { public static ISomeService Current { get; private set; } // This can also be implemented as private static readonly Func&lt;ISomeService&gt; _initializer; public static void SetInitializer(Func&lt;ISomeService&gt; initializer) { _initializer = initializer; Current = initializer(); } } . In this version, changing the initializer changes the singleton object returned. . This and the other 2 implementations also leave one more hole in the codebase. We cannot forget to initialize singletons before using them otherwise it might cause issues. (I guess this is called Temporal Coupling?) . Another approach here is to merge the SomeService and SomeServiceContext class, which although violates the single responsibility principle. . Singleton using IoC Containers . Okay I cheated on this one. You can use the default ServiceProvider provided by .NET or some other IoC (Inversion of Control) container that lets us register an object with a lifetime scope and they provide a Singleton scope as well. . We can then register a dependency on the object and the IoC container will provide us the instance with a Singleton lifetime. That way, the main goal of keeping the object a singleton across the code, is achieved. . Since we are using the IoC container (and registration is usually against an interface), this makes the consuming code easily unit testable. It also doesn‚Äôt violate Single Responsibility Principle since now the creation is the concern of the IoC container. . The con still exists that nothing‚Äôs stopping you to create multiple instances of SomeService in your code. . Modelling the Singleton a Dependency in the Consuming class . There is one more shortcut to reduce the control the dependency in the consuming code for the singleton service while mostly using the default pattern. The consuming code can take the singleton as a dependency in the constructor with default value as the default singleton‚Äôs instance, like so: . public ConsumingService { private SomeService _someServiceInstance; public ConsumingService(SomeService instance = SomeService.Instance) { _someServiceInstance = instance; } } . Now for testability, we can pass a mock for the SomeService code instead of actual service and we get runtime flexibility without adopting the ambient context pattern. . Closing Thoughts . Singleton pattern is quite useful for ambient services, but their default implementation is not that great. Singleton can be a good pattern if: . It doesn‚Äôt have a state which can be modified in different parts of the application and affect the behvior of all the other parts. | You use the different implementations discussed above or a combination of those so that the consuming code is more testable and easy to work with. | The use/dependency of the singleton is not hidden deeply into the code. | Otherwise use the singleton in a code that will not be unit tested. | . And I end with a generic statement, this pattern can be good, given its used wisely! .",
            "url": "https://mitesh1612.github.io/blog/2021/12/10/singleton-antipattern",
            "relUrl": "/2021/12/10/singleton-antipattern",
            "date": " ‚Ä¢ Dec 10, 2021"
        }
        
    
  
    
        ,"post1": {
            "title": "How to Create a Fluent API in C#",
            "content": "If you are a regular C# developer, you must have seen a ‚ÄúFluent‚Äù API/Class commonly, for example while using LINQ . var recentBigCustomers = OrdersList.Where(o =&gt; o.Amount &gt; 1000 &amp;&amp; o.Date &gt;= DateTime.Now.AddDays(-5)) .OrderBy(o =&gt; o.Amount) .Take(10) .Select(o =&gt; o.Customer) .ToList(); . This post will introduce these Fluent APIs, share some common benefits and pitfalls of using such an API and a short tutorial on how to design such a class yourself. . What are Fluent APIs? . So we use MSTest for our testing framework and our assertions for tests look something like this: . [TestMethod] public void ValueOfProperty_ShouldBeEqualTo_Five() { var propertyValue = 5; Assert.AreEqual(propertyValue, 5); // or can be Assert.IsTrue(propertyValue == 5) } . and I was used to seeing all the tests employ similar assertions. But then one fine day, I saw a completed PR that added new tests and their aseertions looked much cleaner, something like this: . [TestMethod] public void ThisTest_ShouldThrow_ArgumentException() { Action a = () =&gt; throw new ArgumentException(&quot;MyMessage&quot;); a.Should() .Throw&lt;ArgumentException&gt;() .And .Message .Should() .Contain(&quot;MyMessage&quot;); } . They were using this library called Fluent Assertions to write their assertions, and although it doesn‚Äôt fundamentally change the test or performance, the code looks readable. There is a similar library to write validations as well, aptly named Fluent Validation. . (Side Note: I know these look a lot like the assertions used in Jest test runs as well). . The name of the library caught my eye, ‚ÄòFluent‚Äô Assertions. So I looked into that library. I was intrigued by the class design for this library and then I recalled, hey this API looks a lot like how LINQ methods are used in C# as well. . So I started reading up on what ‚ÄòFluent‚Äô APIs are. I think Martin Fowler‚Äôs blog post on them is a good introduction, but I am going to take a stab at it as well (although do read that post once you‚Äôre done here). . Sometime or other you must‚Äôve written code like this: . string value = input.ToString(); value = value.ToLower(); value = value.Trim(); value = value.Substring(1, 5); . or the more weirder version where you create multiple temporary values. Since all the string methods return a string, we can write this in a more concise way like this: . string value = input.ToString().ToLower().Trim().Substring(1, 5); . Again, they both do the same thing, but this is more concise and readable according to me. . The other benefit I see of using a Fluent API is great code-completion or ‚ÄúIntellisense‚Äù where the IDE itself can suggest what you should write next and you don‚Äôt need to read through a bunch of API docs to understand what methods a class providers to you. . The intention of designing a ‚Äòfluent‚Äô API is to produce an API that is readable and flows. We can do similar things in with a non-fluent API as well, and granted do it as well, but like in the LINQ example, it would need a lot more lines of code and a bunch of temporary variables. . To be honest, my fascination with them was mostly because it looked cool and interesting to me, and to convert an existing class or code I‚Äôve written to a ‚Äòfluent‚Äô styled API seemed interesting to me. So I‚Äôll skip writing about the practical benefits and pitfalls of creating such an API until after the tutorial. . I thought what better way to learn how to write a Fluent API than to create one yourself. So lets create a simple fluent API from scratch. . I‚Äôll walk through a (mostly) toy problem, and show you two ways to create a Fluent API, a simple way and a more better way to create a Fluent API. . The (toy) Problem . So we usually write a Dockerfile by hand. If you don‚Äôt know what Docker or Dockerfile is (which the toy problem relies a bit on), I‚Äôd recommend quickly going through this amazing video by Fireship that explains it in 100 seconds. Done? Lets move forward then. . So we write a dockerfile by hand, but I wanted to represent a Dockerfile using a C# object, then set the various components for it, like the base image, or the commands I want to run using property setters on that object and finally get the actual text Dockerfile content using a GenerateDockerfile() method. If it sounds complicated, its really not (seeing the class in action soon might help in clarifying it a bit more). . Instead of creating a normal class, I thought of writing it as a ‚ÄúFluent‚Äù class. . Now lets solve the problem. . If you want to see the final result, you can check this repo containing all the code that we are going to write. . The Simple Way to Create a Fluent API . If you look at the Fluent API from the surface, you see that all the methods that are chained together are essentially setters, and these setters return a value, usually the same object back, so more methods can be chained on top of them. This is exactly the simple way to write a fluent API. . Take for example that we want to write a SimpleFluentDockerfileGenerator which works like this: . var dockerFile = SimpleFluentDockerFileGenerator.CreateBuilder() .FromImage(&quot;node:12&quot;) .CopyFiles(&quot;package*.json&quot;, &quot;./&quot;) .RunCommand(&quot;npm install&quot;) .WithEnvironmentVariable(&quot;PORT&quot;,&quot;8080&quot;) .ExposePort(8080) .WithRunCommand(&quot;npm start&quot;) .GenerateDockerFile(); . Here each method would be returning the same object and at last the GenerateDockerFile() would return the text content of the docker file. . If you see, there are two important methods. One is the entry method like CreateBuilder() above or Initialize(). Although not really required (like in LINQ), these are important in setting up the skeleton of the underlying object. The second one is the exit method like ToList() in LINQ or GenerateDockerFile() here which gives the actual result. All the other outputs are partial outputs. . The other important aspect of this fluent class comes from the entry method. The constructor for this class should be private. Why private? Because otherwise a user can just do . var _ = new SimpleFluentDockerFileGenerator() . and we dont want that. So instead we make the constructor private and create a static factory method on the class. . Taking these two principles, we can build a simple version of the Fluent class. Here is a snippet of my Fluent Class: . public class SimpleFluentDockerFileGenerator { private StringBuilder _dockerFileContent; private SimpleFluentDockerFileGenerator() { this._dockerFileContent = new StringBuilder(); } public static SimpleFluentDockerFileGenerator CreateBuilder() { return new SimpleFluentDockerFileGenerator(); } public SimpleFluentDockerFileGenerator FromImage(string imageName) { this._dockerFileContent.AppendLine($&quot;FROM {imageName}&quot;); return this; } } . Here I am using StringBuilder to store the actual string content of the dockerfile and the setters are essentially just adding content to this string builder. (there might be other ways to do it like having a bunch of private properties, but this is the way I chose). As you can see, my constructor is private which initializes a new object and I have a factory method that returns a new object. . You can also see the setter method FromImage that adds content to the object and returns it back, which will enable us to use ‚Äúmethod-chaining‚Äù on our objects. . And really, that‚Äôs it. You can keep adding such methods to add more content and complete the fluent class. Here is a link to my implementation of the same class. . The Problem with this Approach . You might be thinking, hey if creating a Fluent API is this simple, why can I see a lot of content remaining in this post? . While this approach is simple for bootstrapping, can you find an issue by designing your fluent classes in this way? (Just issues in the fluent design only üòÖ) . The major issue is that, there is no guidance on the order of methods a user should use. When the user loads up the object in their IDE and sees all the method suggestion, they don‚Äôt really get a hint of which methods are required to be called or what order they should be called in. So a user can do something like: . var dockerFile = SimpleFluentDockerFileGenerator.CreateBuilder() .CopyFiles(&quot;package*.json&quot;, &quot;./&quot;) .RunCommand(&quot;npm install&quot;) .ExposePort(8080) .WithRunCommand(&quot;npm start&quot;) .GenerateDockerFile(); // Where&#39;s the base image!! . and get a dockerfile without a base image and no error would be thrown. I know we can maybe add validations to ensure its called and stuff like that, but it still doesn‚Äôt solve our other problems like guiding the user about the order of the methods used or bombarding with all the available methods instead of a few available ones. . For example in LINQ, whenever you call the OrderBy() method, that method gives you new methods that you are able to call like .ThenBy() or .ThenByDescending(). It doesn‚Äôt make sense to perform .ThenBy() on its own. . I think the approach we are going to see now is powerful in these aspects. They in a way guide a user on what methods are available, what their natural order should be, and how the user can avoid shooting themselves on the foot by missing calling some important methods. . The Better Way to Create a Fluent API . If you guessed it, great. If not, hey that‚Äôs why I am writing the post right. We are going to use Interfaces to solve all the problems listed above. . Like in the LINQ OrderBy example, when you call OrderBy() on an IEnumerable object, instead of returning another IEnumerable, it returns an object of type IOrderedEnumerable which adds or remove new methods in your method chain. . We are going to create interfaces to limit the next set of methods an object can call or what methods are allowed to be called at a particular time. Then our class would implement our interfaces and the methods would change their return types from the class to the respective interfaces thus giving us the better, guided flow we want. . There are multiple ways regarding the process about designing a fluent API, for example this article shows a great process on it, by defining various grammar rules and then using them to create interfaces. . I am going to try something different. Let‚Äôs try to see the order of methods that we can call using a state diagram [and no I am not using this way to flex on my excalidraw skills, although a man can do both]. Consider the following state diagram. . . As you can see, after you create the builder, the first method should be FromImage() to set the base image. After calling it, we can set any of the other properties like copying files, setting environment variables, running commands, exposing port with their respective methods, but you shouldn‚Äôt be able to get back to set the base image again here. You can cycle through any of the states, i.e. any state should be reachable from any other state. From all of these states, you should be able to reach the penultimate state that can set the container startup command using the WithCommand() method. Again, once you reach there, you shouldnt be able to go back to any previous state (since there is no point of setting any other properties after adding the container startup command). After this, you should only be able to generate the dockerfile. . Let us define these ‚Äústages‚Äù (I am referring to stages as a bunch of states) as . CanSetBaseImage | CanSetContainerProperties | CanAddRunCommand | CanGenerateDockerfile | So we can group our states in our previous state diagram like this: . . Now if we define the transition between these stages as follows, we get this: . . We can now model these stages as interfaces now, and use the transition between these stages to determine the return type of each methods. . So for example, I can create an interface ICanSetBaseImage which will be returned by the Create() class. This will only contain the method to set the base image (the FromImage() method). The return type of this method would be the next interface in the stage transition diagram, ICanSetContainerProperties: . public interface ICanSetBaseImage { public ICanSetContainerProperties FromImage(string image); } . If we look at the end of the stage diagram, we can create an interface called ICanAddRunCommand that will only let the user add the container startup command and that method returns an object of the interface called ICanGenerateDockerfile which only lets you generate the dockerfile now. . public interface ICanAddRunCommand { public ICanGenerateDockerFile WithCommand(string command); } public interface ICanGenerateDockerFile { public string GenerateDockerFile(); } . Now for the ICanSetContainerPropertiesInterface. For all the methods in this interface, they can call their methods again (signified using the loop in the transition diagram) or use the WithCommand method to break out of it. We can model this using inheritance between the interfaces. . public interface ICanSetContainerProperties : ICanAddRunCommand { public ICanSetContainerProperties CopyFiles(string source, string destination); public ICanSetContainerProperties RunCommand(string command); public ICanSetContainerProperties ExposePort(int port); public ICanSetContainerProperties WithEnvironmentVariable(string variableName, string variableValue); } public interface ICanAddRunCommand { public ICanGenerateDockerFile WithCommand(string command); } . This would let the object either call the same methods again or use the WithCommand method to short circuit and get out of the loop. . Once this is done, we can make our class implement all these interfaces as follows: . public class FluentDockerfileGenerator : ICanSetBaseImage, ICanGenerateDockerFile, ICanSetContainerProperties, ICanAddRunCommand { . and then change the return type of the respective methods to the appropriate interface types like: . public class FluentDockerfileGenerator : ICanSetBaseImage, ICanGenerateDockerFile, ICanSetContainerProperties, ICanAddRunCommand { private StringBuilder _dockerFileContent; private FluentDockerfileGenerator() // The private constructor { this._dockerFileContent = new StringBuilder(); } public static ICanSetBaseImage CreateBuilder() { return new FluentDockerfileGenerator(); } public ICanSetContainerProperties FromImage(string imageName) { this._dockerFileContent.AppendLine($&quot;FROM {imageName}&quot;); return this; } . You can refer to this link to the repo containing the full code for this class. . Now if we use the same fluent class, we get in a way guided tour of the natural order of methods and the methods we have at our disposal. Here are a few examples of the awesome code completion we get with this fluent class. . . Do take a look at the complete code in the repository, and contributions are always welcome. üòÄ . So what did we lose? . Requirement of Domain Knowledge . The best start to discussing the benefits and pitfalls of creating a fluent API comes for the example itself that we took. As you can see, designing a fluent API is really domain specific like the example was really specific to the domain of generating a dockerfile and this is what Martin Fowler believes as well. This requirement of the domain knowledge forces you to think as a user of your API, how well your API communicates with the users and is critical to the success of it as well. This also trickles down to the users of the API, but only if the API is not well designed I believe. If the user is not well versed in the domain, they would be lost trying to combine enigmatic methods, stitching things together until they work. Although a well defined fluent API can remediate a lot of it. . Design Roadblocks . Usually you are likely to hit design roadblocks while designing a fluent API (as with any API design in general), since this is not a design pattern nor a framework and I‚Äôll try to outline a few common deficiencies if you decide to implement such an API. . Gratuitous Extension Methods, where you keep adding new methods to avoid the cost of refactoring | Context Confusion, where sometimes new methods totally unrelated to the original context are added | Exception MisManagement, where you have to write a lot more code to handle and manage exceptions better | Viscous Interface, where due to the above 3, the interface loses its fluency over time. | For more details on this problem, I‚Äôd recommend going through this article . Or this one very wonderful comment I found on this wonderful YouTube video on how to write a Fluent API. . . And what did we gain? . I think the biggest benefit of a Fluent Interface, when designed well, only let you use what makes sense to use in the current context. In a properly designed fluent interface, you shouldn‚Äôt be able to: . Call the methods in the wrong order | Be confused on where to start | Be confused about where to go next | Call methods in a specific chain partially | . Fluent Interface design gives the developer a techinique for building code that is designed with the intent that other people might actually be reading it. This also creates a new mindset where instead of thinking ‚Äúwhat data does this object have‚Äù, you are forced to think about ‚Äúwhat kinds of things this object can do?‚Äù . These benefits may seem superflous or not worth the effort required to build such an interface API and I totally get that. . To me, as I said, it looks cool, and that was a reason enough to me to learn a bit more about them, and I hope this post does the same for you. . That‚Äôs it for this one. You can always share your thoughts on this by leaving a comment on this post. If you liked what you read, you can try reading some other posts on my blog as well, and you can also connect with me on my socials. .",
            "url": "https://mitesh1612.github.io/blog/2021/08/11/how-to-design-fluent-api",
            "relUrl": "/2021/08/11/how-to-design-fluent-api",
            "date": " ‚Ä¢ Aug 11, 2021"
        }
        
    
  
    
        ,"post2": {
            "title": "How to create a custom C# Attribute to Inject Extra Behavior in Tests",
            "content": "So the title definitely sounds way more complicated than it has any right to, so I‚Äôd rather suggest you to read through this post to completely understand the problem and the solution I went with. As I always says, the solution seems trivial, and you might already know something on the lines of it, but it took me a good amount of researching and experimenting to get it to work, so I thought I should share it. . Also do note, that this doesn‚Äôt solve the general problem of injecting extra behavior using C# attributes or use C# attributes as decorators like in Python, that I still feel requires a lot more effort. This just solves the problem for writing tests. Apologies if you stumbled here and this doesn‚Äôt solve your problem. üòÄ . The Problem Statement . So we (as in me and my team) created a custom feature flag service for ourselves (keep an eye out for a future posts regarding my findings about the whole ordeal). Now we wanted to enhance our existing test framework to make our existing tests work with feature flags. We wanted options to run our test where the developer can declare/specify the expected state of the feature flag before the test, and it will be set as such, and wanted the experience to author new tests and modify existing tests as seamless as possible. . So to start with we created some helper methods, and instructed everyone to ‚Äúwrap‚Äù their test code between the required helper methods like so: . [TestMethod] public void SomeRandomTest() { FeatureFlagHelpers.SetFeatureFlag(featureName, featureState); // Actual test code FeatureFlagHepers.UnsetFeatureFlag(featureName, featureState); } . While this did serve the purpose we wanted, this process is cumbersome for devs to author tests with the feature flag service. Moreover I feel this is error prone. What if the dev forgot to unset the feature flag in the excitement of writing those bunch of assertions? It can interfere with other tests as well. . We wanted something simpler, and similar to something like Python decorators, where in we can just use an attribute to decorate the test with desired configuration for the feature flag, and that should be it. Something like: . [FeatureFlag(&quot;featureA&quot;, true)] [TestMethod] public void SomeRandomTest() { // Actual test code } . Our Limitations . So our testing framework is simple, and we use MSTest as our testing framework. We wanted to avoid any complex libraries or making the test framework more complex (like adding super reflection heavy code?). . I also wanted to use C# Attributes, because usually test authors are already in the habit of using attributes to annotate the test methods. . I read up on C# Attributes and I realised, they are quite different from their Python counterparts. While Python decorators can be used to add extra behavior to existing methods, C# attributes just add metadata and are basically useless for that purpose. (One post laughingly also called them as glorified comments). . The Journey to the Solution . So from my earlier specification, this seemed like the perfect fit for the decorator design pattern. It is essentially used to add extra behavior to existing classes/code. . I quickly threw up a simple higher order function type solution for my problem like this: . public static void RunFeatureFlagTest(Action testToRun, string featureName, bool featureState) { FeatureFlagHelper.SetFeatureFlag(featureName, featureState); testToRun(); FeatureFlagHelper.UnsetFeatureFlag(featureName, featureState); } [TestMethod] public void SomeRandomTestRunner() { RunFeatureFlagTest(SomeRandomTest, &quot;featureA&quot;, true); } public void SomeRandomTest() { // testing code } . So now we are basically wrapping the execution of test with a higher order function (eerily similar to how Python decorators are written). So for a developer, now they have to write their test code. Then create a runner function invoking the test using the higher order function we wrote. I still feel this is a reasonable trade-off and definitely better than the original way, but I wanted to look further for solutions. . So I started reading on how to achieve a lot of things using C# Attributes. I read up about Aspect Oriented Programming and how PostSharp can be used to achieve exactly what I wanted using C# attributes. But as I said, I wanted to avoid using any extra library, and thus ignored this suggestion. . It felt like we can solve this problem using reflection, or hey, even the new source generators feature of .NET seems like a great way to do it, like this and this articles were doing for themselves. But I am not good at reflection, yet üòâ and source generators are a preview feature at the time of writing. (Although they also seemed quite complex to me). . Then I went to basics, looked into how the MS Test Framework uses the [TestMethod] attribute. . Hacking with the TestMethod Attribute . So I realised that the second attribute [DataTestMethod] basically inherits the [TestMethod] attribute. This got me thinking, what if I also inherit from the [TestMethod] attribute and create my own custom attribute. Hopefully MS Test recognizes methods decorated with that attribute as test methods as well and runs them using VS Test Explorer or dotnet test command. Boy I was right! . The TestMethodAttribute class looks something like this: . public TestMethodAttribute : Attribute { public TestMethodAttribute() { } public TestResult[] virtual Execute() { } // This executes the test and returns test results of type TestResult } . Interestingly, the virtual keyword preceeding the Execute() method downright indicated that this method was made to be overriden and do interesting things with (I know I know, C# basic feature, but boy was I happy for it being helpful in this real life situation). . Awesome, so I need to inherit this attribute, inject whatever behavior I wanted before calling the base‚Äôs Execute method and ideally my problem is solved. This is exactly the solution. So my final test attribute looked like this. . public FeatureFlagTestAttribute : TestMethodAttribute { private string featureFlagName; private bool featureFlagState; public FeatureFlagTestAttribute(string ffName, bool ffState) { featureFlagName = ffName; featureFlagState = ffState; } public override TestResult[] Execute() { FeatureFlagHelper.SetFeatureFlag(featureFlagName, featureFlagState); var results = base.Execute(); FeatureFlagHelper.UnsetFeatureFlag(featureFlagName, featureFlagState); return results; } } . And now the tests can easily be authored as follows: . [FeatureFlagTest(&quot;featureA&quot;, true)] public void SomeRandomTest() { } // testing code. . This is exactly what I wanted. . I really prefer this approach, obviously because it exactly matches the spec we wanted for our testing framework, looks cleaner, and can be used to do other things as well (like logging stuff etc). This seems like the tip of the iceberg, and I feel more exploration with the existing attributes that MSTest Framework provides, we can definitely supercharge our testing game. . There are obvious limitations to this as well. For example, some of our helpers were not static methods. They are instance methods of the base class from which all test classes inherit, and I dont prefer sending the whole instance of the test class to my attribute class. . Closing Thoughts . I know this is again a specific problem, for people using MS Test Framework, but I think similar things could be achieved with NUnit or other testing frameworks as well. If you have thoughts or suggestions on how this problem could have been solved in a better way, feel free to sound off in the comments. If you have any other feedback, like this article seeming like an overly long introduction to the TestMethodAttribute, feel free to share that as well in the comments as well. Hope this article was fun for you, catch you in the next one. üòÄ .",
            "url": "https://mitesh1612.github.io/blog/2021/07/01/custom-test-attribute",
            "relUrl": "/2021/07/01/custom-test-attribute",
            "date": " ‚Ä¢ Jul 1, 2021"
        }
        
    
  
    
        ,"post3": {
            "title": "How to have Multiple Custom Script Extensions for Azure Virtual Machines in One ARM Template Deployment",
            "content": "Yes, I‚Äôm back with yet another Azure specific problem, but judging from the number of issues this problem is mentioned in, and the lack of solutions for it, I thought this solution is worth sharing more. I will not go over the concepts of Azure Virtual Machines or the Custom Script Extensions for Azure Virtual Machines. Microsoft gives a pretty great documentation for those, which you can read here. . For the gist of it, Custom Script Extensions are used as post deployment configuration, or any other installation or configuration tasks after deploying a Virtual Machine, which can be added in an ARM template deployment for that Virtual Machine (VM). Let‚Äôs look at the problem I was trying to solve. . My Problem . So I was running 2 different scripts performing 2 different tasks in the deployment, but at different stages of the deployment. Consider the following diagram. (Sorry for the vague names, I dont believe the actual details matter here more than the problem) . . So I was performing a task using Script A at one stage of the deployment. Once Script A finishes execution, I need to deploy a resource R. R‚Äôs deployment depends on the task done by Script A. Once R is deployed, I wanted to run another Script B. B depends on R‚Äôs deployment. (Assume A is some pre-configuration for resource R, and B is some post-configuration needing resource R). . Its not that Azure doesnt support running multiple scripts, they do infact as specified in this example. My issue was that I wanted to run Script B (the second script) at a later stage in the deployment. . So naively, say you tried adding these 2 scripts in your ARM template, like this: . [ { &quot;type&quot;: &quot;Microsoft.Compute/virtualMachines/extensions&quot;, &quot;apiVersion&quot;: &quot;2020-12-01&quot;, &quot;name&quot;: &quot;[concat(variables(&#39;vmName&#39;),&#39;/&#39;, &#39;ScriptA&#39;)]&quot;, &quot;location&quot;: &quot;[parameters(&#39;location&#39;)]&quot;, &quot;dependsOn&quot;: [ &quot;[concat(&#39;Microsoft.Compute/virtualMachines/&#39;,variables(&#39;vmName&#39;))]&quot; ], &quot;properties&quot;: { &quot;publisher&quot;: &quot;Microsoft.Compute&quot;, &quot;type&quot;: &quot;CustomScriptExtension&quot;, &quot;typeHandlerVersion&quot;: &quot;1.7&quot;, &quot;autoUpgradeMinorVersion&quot;: true, &quot;settings&quot;: { &quot;fileUris&quot;: [ &quot;https://SomePublicUrl/HavingThe/ScriptA.ps1&quot; ], &quot;commandToExecute&quot;: &quot;powershell.exe -ExecutionPolicy Unrestricted -File ScriptA.ps1&quot; } } }, // Omitting R&#39;s deployment { &quot;type&quot;: &quot;Microsoft.Compute/virtualMachines/extensions&quot;, &quot;apiVersion&quot;: &quot;2020-12-01&quot;, &quot;name&quot;: &quot;[concat(variables(&#39;vmName&#39;),&#39;/&#39;, &#39;ScriptB&#39;)]&quot;, &quot;location&quot;: &quot;[parameters(&#39;location&#39;)]&quot;, &quot;dependsOn&quot;: [ &quot;[concat(&#39;Microsoft.SomeResource/&#39;,variables(&#39;resourceR&#39;))]&quot; ], &quot;properties&quot;: { &quot;publisher&quot;: &quot;Microsoft.Compute&quot;, &quot;type&quot;: &quot;CustomScriptExtension&quot;, &quot;typeHandlerVersion&quot;: &quot;1.7&quot;, &quot;autoUpgradeMinorVersion&quot;: true, &quot;settings&quot;: { &quot;fileUris&quot;: [ &quot;https://SomePublicUrl/HavingThe/ScriptB.ps1&quot; ], &quot;commandToExecute&quot;: &quot;powershell.exe -ExecutionPolicy Unrestricted -File ScriptB.ps1&quot; } } } ] . and I am sure you must have, then your template will not fail validation and start deployment. But when it reaches the second script, you are greeted with this error: . ‚ÄúMultiple VMExtensions per handler not supported for OS type ‚ÄòWindows‚Äô. Extension ‚ÄòScriptA‚Äô with handler ‚ÄòMicrosoft.Compute.CustomScriptExtension‚Äô already added.‚Äù . Ah, as Microsoft says, you cant use multiple custom script extension in one ARM deployment, or can you? . The Solution . So Microsoft technically says you can‚Äôt have multiple VM Extensions in one deployment. But there is a hack here. If you remove the first custom script extension during deployment, before the second script extension is deployed, you can add another custom script extension! üòÄ This is it, this is how this problem is solved. If this was enough for you, thanks for reading till here. Follow along on how to actually do it. . So in our problems context, we need to remove the custom script extension for script A, before script B is deployed. There are multiple ways to go about it: . ScriptA contains the code to remove itself. | Remove ScriptA using some other way during the deployment. | Obviously 1 is a simpler way, you can add a az cli or AzPowerShell command at the end of your script to reomve their custom script extension. I dont prefer that way, because then your script needs all these details (name of your resource group, name of the VM, name of the Custom Script extension etc.) and these dont feel like generic scripts (which are better and easier to test/develop). So lets focus on way 2. . Since ARM deployments dont allow you to make any other calls except Create Resource calls, how do you delete a resource in an ARM template deployment? We can use Deployment Scripts and an az CLI command to delete the extension for script A. We can also use a clever depends on relationship to make sure everything happens when we want it to, as shown in the diagram below. Moreover, you can also share a managed identity between the Virtual Machine and Deployment Script, if your custom script extension also needs an identity. . . Now all that remains is to write this down in an ARM template. That template might look like as follows. . Enough talk, show me the ARM template . So here‚Äôs how my template works. First we will create the extension for script A. Then we deploy R and the deployment script to remove A, which is written as an azure cli script, both dependent on A. And then we add the extension for script B. (I‚Äôve only shown the resources section for brevity). . [ { &quot;type&quot;: &quot;Microsoft.Compute/virtualMachines/extensions&quot;, &quot;apiVersion&quot;: &quot;2020-12-01&quot;, &quot;name&quot;: &quot;[concat(variables(&#39;vmName&#39;),&#39;/&#39;, &#39;ScriptA&#39;)]&quot;, &quot;location&quot;: &quot;[parameters(&#39;location&#39;)]&quot;, &quot;dependsOn&quot;: [ &quot;[concat(&#39;Microsoft.Compute/virtualMachines/&#39;,variables(&#39;vmName&#39;))]&quot; ], &quot;properties&quot;: { &quot;publisher&quot;: &quot;Microsoft.Compute&quot;, &quot;type&quot;: &quot;CustomScriptExtension&quot;, &quot;typeHandlerVersion&quot;: &quot;1.7&quot;, &quot;autoUpgradeMinorVersion&quot;: true, &quot;settings&quot;: { &quot;fileUris&quot;: [ &quot;https://SomePublicUrl/HavingThe/ScriptA.ps1&quot; ], &quot;commandToExecute&quot;: &quot;powershell.exe -ExecutionPolicy Unrestricted -File ScriptA.ps1&quot; } } }, // Omitting R&#39;s deployment { &quot;type&quot;: &quot;Microsoft.Resources/deploymentScripts&quot;, &quot;apiVersion&quot;: &quot;2020-10-01&quot;, &quot;name&quot;: &quot;RemoveScriptA&quot;, &quot;location&quot;: &quot;[resourceGroup().location]&quot;, &quot;kind&quot;: &quot;AzureCLI&quot;, &quot;identity&quot;: { &quot;type&quot;: &quot;UserAssigned&quot;, &quot;userAssignedIdentities&quot;: { &quot;[resourceId(&#39;Microsoft.ManagedIdentity/userAssignedIdentities&#39;, parameters(&#39;identityName&#39;))]&quot;: {} } }, &quot;dependsOn&quot;: [ &quot;[resourceId(&#39;Microsoft.Compute/virtualMachines/extensions&#39;, variables(&#39;vmName&#39;),&#39;ScriptA&#39;]&quot; ], &quot;properties&quot;: { &quot;forceUpdateTag&quot;: &quot;[parameters(&#39;utcValue&#39;)]&quot;, // To force run script on redeployment &quot;AzCliVersion&quot;: &quot;2.2.0&quot;, &quot;timeout&quot;: &quot;PT30M&quot;, &quot;arguments&quot;: &quot;[concat(variables(&#39;vmName&#39;), &#39; &#39;, resourceGroup().name)]&quot;, &quot;scriptContent&quot;: &quot;az vm extension delete -g $2 --vm-name $1 -n ScriptA&quot;, // Az CLI Command to remove an extension &quot;cleanupPreference&quot;: &quot;OnSuccess&quot;, &quot;retentionInterval&quot;: &quot;P1D&quot; } } { &quot;type&quot;: &quot;Microsoft.Compute/virtualMachines/extensions&quot;, &quot;apiVersion&quot;: &quot;2020-12-01&quot;, &quot;name&quot;: &quot;[concat(variables(&#39;vmName&#39;),&#39;/&#39;, &#39;ScriptB&#39;)]&quot;, &quot;location&quot;: &quot;[parameters(&#39;location&#39;)]&quot;, &quot;dependsOn&quot;: [ &quot;[concat(&#39;Microsoft.SomeResource/&#39;,variables(&#39;resourceR&#39;))]&quot;, &quot;[resourceId(&#39;Microsoft.Resources/deploymentScripts&#39;, &#39;RemoveScriptA&#39;]&quot; ], &quot;properties&quot;: { &quot;publisher&quot;: &quot;Microsoft.Compute&quot;, &quot;type&quot;: &quot;CustomScriptExtension&quot;, &quot;typeHandlerVersion&quot;: &quot;1.7&quot;, &quot;autoUpgradeMinorVersion&quot;: true, &quot;settings&quot;: { &quot;fileUris&quot;: [ &quot;https://SomePublicUrl/HavingThe/ScriptB.ps1&quot; ], &quot;commandToExecute&quot;: &quot;powershell.exe -ExecutionPolicy Unrestricted -File ScriptB.ps1&quot; } } } ] . This is almost the same template that I used and it works perfectly. . That‚Äôs it for this one. I hope my post helps you if you were also trying to achieve something similar. If you feel this can be achieved in a better or different way, feel free to ‚Äú@‚Äù me at my socials or comment on my blog. See you in the next one. .",
            "url": "https://mitesh1612.github.io/blog/2021/05/14/vms-with-multiple-extensions",
            "relUrl": "/2021/05/14/vms-with-multiple-extensions",
            "date": " ‚Ä¢ May 14, 2021"
        }
        
    
  
    
        ,"post4": {
            "title": "How to deploy Azure Blob Container with files using ARM templates",
            "content": "I know, I know, this is a really specific problem. But one that I encountered, and when I Googled/Bing‚Äôd it, I was not able to find any way to do it. I was able to fix it, and you might too, but I thought I should use my blog to share how I solved it, hoping it might come in handy who wants to achieve something similar. If you have alternate or downright better ways to do it, please feel free to add them in comments. . So I was writing some ARM (Azure Resource Manager) templates and I wanted to create a Azure Data Lake Storage Account (I‚Äôll use ADLS subsequently, since the full name is a handful otherwise) and a blob container as well. But I wanted this template to also populate the blob container with some existing files that lie at a public URL. . Usually, for this, first you need to create the Azure Storage Account and Container using an ARM template and then copy your files manually using any of the the various options available (Upload from portal, Storage Explorer, Azure CLI, Azure Powershell), but I wanted my ARM template to do this, since I wanted to automate of creating an already ‚Äúinitialized‚Äù container state without manual intervention. For that, I dipped my toes into a functionality ARM provides called Deployment Scripts. . What are Deployment Scripts? . Basically, Deployment Scripts provide you an option to run certain scripts for (mostly dataplane) operations as part of your ARM deployment. Those are useful if you want to perform some operation everytime with an ARM deployment. At the time of writing, ARM supports 2 variants of Deployment Scripts: . Az Powershell Deployment Scripts - If Powershell is your scripting language of choice and you‚Äôre comfy with Azure Powershell commands | Az CLI Deployment Scripts - If you‚Äôre more of a ‚Äúbash‚Äù person and use Az CLI a lot | I don‚Äôt have a preference of bash vs. Powershell, but I am well versed with Az CLI and hence I chose option 2. . The overview of this solution is simple. First create a storage account and a container using an ARM template, and then run a deployment script that downloads required files and uploads them to this created storage account. Simple, right? Since deployment scripts runtime is authenticated, I dont need to perform manual login, and it just works! ‚ô• . I don‚Äôt want this post to be a puff piece on deployment scripts, or a tutorial on how to use them. I believe Azure has some nicely written documentation for it, so if you need reference on deployment scripts, please use the documentation. Follow along for the actual template I used. . Enough talk, show me the ARM template . So here‚Äôs how my template works. First we will create a storage account, and a container for that storage account. This is textbook template for ADLS, nothing special here. (I‚Äôve only shown the resources section for brevity) . [ { &quot;type&quot;: &quot;Microsoft.Storage/storageAccounts&quot;, &quot;apiVersion&quot;: &quot;2019-06-01&quot;, &quot;name&quot;: &quot;[parameters(&#39;storageAccountName&#39;)]&quot;, &quot;location&quot;: &quot;[resourceGroup().location]&quot;, &quot;dependsOn&quot;: [], &quot;sku&quot;: { &quot;name&quot;: &quot;Standard_RAGRS&quot; }, &quot;kind&quot;: &quot;StorageV2&quot;, &quot;properties&quot;: { &quot;accessTier&quot;: &quot;Hot&quot;, &quot;supportsHttpsTrafficOnly&quot;: true, &quot;isHnsEnabled&quot;: true } }, { &quot;type&quot;: &quot;Microsoft.Storage/storageAccounts/blobServices&quot;, &quot;apiVersion&quot;: &quot;2019-04-01&quot;, &quot;name&quot;: &quot;[concat(variables(&#39;storageAccountName&#39;), &#39;/default&#39;)]&quot;, &quot;dependsOn&quot;: [ &quot;[resourceId(&#39;Microsoft.Storage/storageAccounts&#39;, parameters(&#39;storageAccountName&#39;))]&quot; ], &quot;properties&quot;: { &quot;cors&quot;: { &quot;corsRules&quot;: [] }, &quot;deleteRetentionPolicy&quot;: { &quot;enabled&quot;: false } } }, { &quot;type&quot;: &quot;Microsoft.Storage/storageAccounts/blobServices/containers&quot;, &quot;apiVersion&quot;: &quot;2019-04-01&quot;, &quot;name&quot;: &quot;[concat(parameters(&#39;storageAccountName&#39;), &#39;/default/&#39;, parameters(&#39;containerName&#39;))]&quot;, &quot;dependsOn&quot;: [ &quot;[resourceId(&#39;Microsoft.Storage/storageAccounts/blobServices&#39;, parameters(&#39;storageAccountName&#39;), &#39;default&#39;)]&quot;, &quot;[resourceId(&#39;Microsoft.Storage/storageAccounts&#39;, parameters(&#39;storageAccountName&#39;))]&quot; ], &quot;properties&quot;: { &quot;publicAccess&quot;: &quot;None&quot; } }, ] . Now we will add a deployment script resource. Since the deployment script needs a logged in user‚Äôs context, we also need to create a User Assigned Managed Identity. After that, we also need to give the managed identity appropriate access so that it can perform required operations. For simplicity, I am going to give Contributor access to the identity on the resource group this template is being deployed to. The following template achieves that . [ { &quot;type&quot;: &quot;Microsoft.ManagedIdentity/userAssignedIdentities&quot;, &quot;apiVersion&quot;: &quot;2018-11-30&quot;, &quot;name&quot;: &quot;[parameters(&#39;identityName&#39;)]&quot;, &quot;location&quot;: &quot;[resourceGroup().location]&quot; }, { &quot;type&quot;: &quot;Microsoft.Authorization/roleAssignments&quot;, &quot;apiVersion&quot;: &quot;2018-09-01-preview&quot;, &quot;name&quot;: &quot;[variables(&#39;bootstrapRoleAssignmentId&#39;)]&quot;, // This is just a random string &quot;dependsOn&quot;: [ &quot;[resourceId(&#39;Microsoft.ManagedIdentity/userAssignedIdentities&#39;, parameters(&#39;identityName&#39;))]&quot; ], &quot;properties&quot;: { &quot;roleDefinitionId&quot;: &quot;[variables(&#39;contributorRoleDefinitionId&#39;)]&quot;, &quot;principalId&quot;: &quot;[reference(resourceId(&#39;Microsoft.ManagedIdentity/userAssignedIdentities&#39;, parameters(&#39;identityName&#39;)), &#39;2018-11-30&#39;).principalId]&quot;, &quot;scope&quot;: &quot;[resourceGroup().id]&quot;, &quot;principalType&quot;: &quot;ServicePrincipal&quot; } }, ] . (This is line by line example that Azure providers in their documentation with minor tweaks for clarity purposes btw.) . Now the actual solution to our problem (sorry for the long boilerplate). . We‚Äôll create a deployment script, and provide the identity above for it to use. Then we will download our data/files using a wget command. Then we can use any of the methods az storage blob CLI commands provide to upload it to the container. In my case, I had a directory with about 20 files to put into the container. So I zipped them and made a public URL of them to download. Then I unzip it in the deployment script and use the upload-batch command to upload the whole unzipped directory. The following template does that. Note I am using an inline script here, since the script was small, and I actually wanted to show the script contents (The important part here is the script, rest is just minor details). . { &quot;type&quot;: &quot;Microsoft.Resources/deploymentScripts&quot;, &quot;apiVersion&quot;: &quot;2020-10-01&quot;, &quot;name&quot;: &quot;UploadFilesToADLS&quot;, &quot;location&quot;: &quot;[resourceGroup().location]&quot;, &quot;kind&quot;: &quot;AzureCLI&quot;, &quot;identity&quot;: { &quot;type&quot;: &quot;UserAssigned&quot;, &quot;userAssignedIdentities&quot;: { &quot;[resourceId(&#39;Microsoft.ManagedIdentity/userAssignedIdentities&#39;, parameters(&#39;identityName&#39;))]&quot;: {} } }, &quot;dependsOn&quot;: [ &quot;[resourceId(&#39;Microsoft.ManagedIdentity/userAssignedIdentities&#39;, parameters(&#39;identityName&#39;))]&quot;, &quot;[resourceId(&#39;Microsoft.Storage/storageAccounts/blobServices/containers&#39;, parameters(&#39;storageAccountName&#39;), &#39;default&#39;, parameters(&#39;containerName&#39;))]&quot; // Should run this script only when container actually gets created ], &quot;properties&quot;: { &quot;forceUpdateTag&quot;: &quot;[parameters(&#39;utcValue&#39;)]&quot;, // To force run script on redeployment &quot;AzCliVersion&quot;: &quot;2.2.0&quot;, &quot;timeout&quot;: &quot;PT30M&quot;, &quot;arguments&quot;: &quot;[concat(parameters(&#39;storageAccountName&#39;), &#39; &#39;, parameters(&#39;containerName&#39;))]&quot;, &quot;scriptContent&quot;: &quot;wget -O files.zip &#39;https://some-public-url/files.zip&#39; ; unzip files.zip ; az storage blob upload-batch -d $2 -s datafolder --account-name $1&quot;, &quot;cleanupPreference&quot;: &quot;OnSuccess&quot;, &quot;retentionInterval&quot;: &quot;P1D&quot; } } . We are passing the name of the storage account and the name of the container as parameters to this deployment script. You can also parameterize the file URL if needed. . The fun part here is, you can also use az storage commands to download files from an ADLS account in the script and then upload them to the new ADLS (if you dont want to put your files at a public URL). In that case, you will need to give the managed identity access to that ADLS and tweak the script accordingly. And speaking of the managed identity, you can also limit the permissions to this identity if Contributor seems too broad of a permission. . That‚Äôs it for this one. I hope my post helps you if you have to automate something similar. If you feel this can be achieved in a better or different way, feel free to ‚Äú@‚Äù me at my socials or comment on my blog. See you in the next one. .",
            "url": "https://mitesh1612.github.io/blog/2021/03/26/storage-container-with-files",
            "relUrl": "/2021/03/26/storage-container-with-files",
            "date": " ‚Ä¢ Mar 26, 2021"
        }
        
    
  
    
        ,"post5": {
            "title": "Hello Xamarin (And 2021)",
            "content": "Hey there. So I planned to do this, at least not 15 days later into the year. 2020 has been a mess of an year and eventhough, on the surface, the start might not be awesome, I have better hopes and goals for 2021. One of these goals is to write even more in this blog (considering I‚Äôve written, what like 6 posts in 5 months?). We will see how that pans out in the future, but here is one update that I‚Äôd like to share, and the topic you might see future posts on. . So I wanted to write an Android App, for a personal usecase. It‚Äôs not that esoteric and an app already exists for it. Then why write one? Because the app required a purchase to keep working and I had a feeling I can create the same app, without the need of any purchase or ads. Now I don‚Äôt really plan to distribute this app on Google Play Store, so other than my ‚Äúman-hours‚Äù and time spent in creating it, I wouldn‚Äôt be having any kind of monetary investment in this. Since its a personal usecase and the app will be written by me only, I am okay with installing it on my phone. So then came the first hurdle in this journey, how to start and which tech stack to use? . The Options . Now I know, the title kindo spoils my end choice, but I‚Äôll still like to share why I choose Xamarin. Since I know nothing about app development and not really good at UI development as well, I wanted an easy entry point for my first ever Android App. (Yes, I‚Äôve gotten around doing my Bachelors and Masters without writing a single Android app üòã). I knew a fair amount of JavaScript and TypeScript, but I am not good at UI development, so I skipped React-Native, eventhough it being one of my primary choices (since the library code I wanted to write could be written in JS/TS easily) and I didnt wanna learn, yet another JS framework, so skipped Ionic and Cordova as well. I didn‚Äôt want to learn yet another language as well, since currently my focus has been on how to write good and maintainable code in the languages I know, so eventhough being similar to JavaScript, I skipped Dart &amp; Flutter. I skipped Kotlin, and all the other native Android stuff Google provides, alongwith Android Studio, primarily because Android Studio is very resource intensive and might just kill my laptop. Plus again, I didn‚Äôt wanna learn another language (Kotlin). . If you‚Äôve been following the blog, you might have noticed that I have been working in C# from some time. I remembered Microsoft creating Xamarin for cross platform app development. Hence I chose Xamarin. I already have Visual Studio on my laptop, I need to practice writing better C# code anyways. Plus I wanted to do all the processing required for this app on-device and not rely on some web service (since they can add/create costs), I thought it would be a good experience in writing that code in C# and learn how to do ‚Äúmore‚Äù using C#. Hence I chose Xamarin as my tool (or framework?) of choice. . Where I am learning Xamarin from? . So I had heard about Xamarin University when I first read about Xamarin. But then I saw that it got closed and some of its content was available on YouTube and their public GitHub repo which is archived. Now I didn‚Äôt have much time to go through various public repos and old videos, plus I prefer reading + doing a tad bit more for learning. So I went to the place where Xamarin University went, Microsoft Learn. I saw two main modules that I feel should give me a good enough intro to Xamarin. First is the module Build Mobile Apps with Xamarin and then I plan to continue with Customize Xamarin Forms App &amp; Advanced Features. Along the way I also plan to read a few books (and if needed watch a video or two). In terms of books, I have two on my radar. First is Xamarin in Action to understand the MVVM pattern and how to write good and testable code. Second is Xamarin Projects to see examples of how to build certain things with Xamarin in action. . On a side note . I know my reasons/issues for choosing Xamarin are weird, and superficial and might not even match to your reasons/issues. Kotlin might be better if you are planning to do full time Android Development, React Native if you already know React and JS well and Dart if you want to learn an awesome new tooling. That being said, do give Xamarin a try. . I‚Äôll definitely keep you, my readers updated with my experience and pitfalls of using Xamarin and C#, and my progress with this app I‚Äôm trying to write. . Do comment if you have suggestions for resources to learn Xamarin, or if you want to share your experience of using Xamarin. Until next time. üòÄ .",
            "url": "https://mitesh1612.github.io/blog/2021/01/16/welcome-xamarin",
            "relUrl": "/2021/01/16/welcome-xamarin",
            "date": " ‚Ä¢ Jan 16, 2021"
        }
        
    
  
    
        ,"post6": {
            "title": "Blog Update 1",
            "content": "Hey there. It‚Äôs been a long time since I wrote a new post like this one (or tbh any post). I wanted to share an update to my blog. . If you‚Äôve been a reader previously (which is higly unlikely), you may have seen that the blog looks quite different now. I changed my blog tech stack from Gatsby to Fastpages and considered sharing my reasons and experience in the process. Now you can comment on my blog posts, search posts using a search feature and even find posts based on categories. . Choosing the right tech stack . I created the v1 of my blog using Gatsby JS and it resounded to a time where I was learning about JS and React JS. But customizing the blog and adding features like Comments and Search was not trivial using the theme I used and I didn‚Äôt want to change my theme, because it looked wonderful. . Then I tried switching to Hugo, which I felt was a bit more easy to customize and blazingly fast, but again, I had to setup comments and search manually. . Then I saw the release of fastpages by fastai and thought whether this could be a right choice for my blogging needs or not. I loved how they let you setup comments using a simple config changes, has search and categorization using tags built in. It serves all my syntax highlighting and adding LaTeX Math Equations to my blog needs, lets me share Jupyter notebooks as blog posts and sets up CI quite easily. I tried it for my blog and I loved the experience, so I changed the tech stack for my blog, and while I am not completely okay with how few things work in fastpages, it serves my basic blogging needs quite well. I will share the issues that I encounter with Fastpages in this post and be on the lookout for a future post on how to create your own blog using Fastpages . My Issues with Fastpages . While I love fastpages in terms of the features it provides, here are some issues I have with it. Do note that these are a bit subjective and might not be completely relevant to you, but here it is: . The Jekyll Stack and Developer Experience is not fun. Fastpages uses Jekyll to generate this site and I do not know that or Ruby yet. Plus to locally run this blog, I need docker etc in my system, which although not a big deal, its one I‚Äôd like to avoid, vs a single executable for Hugo or a npm CLI tool in case of Gatsby. | Theme customization options are limited. Again I feel this is something I endure more since I don‚Äôt know Jekyll, but if you want your blog to look unique or want to change the whole look as easy as flipping to a different theme, it‚Äôs not possible, until you know how to tinker with Jekyll and custom css files. | Referencing images. I had to change my image references from [Alt Text](Image-Location) to ![](/blog/images/Image-Location). Gatsby had a separate image folder for each posts versus fast pages having a single location for all the images. I will try to fix this issue if possible though. | All these issues aside, Fastpages is a great way to setup a blog easily with most of features one might need, and you should definitely consider it to set up a statically generated blog hosted on GitHub pages. Do try it out and share your experiences in the comments below. .",
            "url": "https://mitesh1612.github.io/blog/2020/12/24/blog-update-1",
            "relUrl": "/2020/12/24/blog-update-1",
            "date": " ‚Ä¢ Dec 24, 2020"
        }
        
    
  
    
        ,"post7": {
            "title": "The different kinds of Unit Tests",
            "content": "In my previous post, I talked about various kinds of Tests. As a developer working for a mature codebase, the most common tests that you will need to write are Unit Tests, and its a good practice tbh. . While working on my team‚Äôs project (which was in a fairly early stage), I saw various kinds of Unit Tests and researched a bit myself on how to write some good UTs. Today I plan to share the same in this post. I‚Äôll talk about various types of Unit Tests and when you might want to use each. . A quick disclaimer here. Since I work on a C# project, and have the most experience in writing C# (or Python tests), my examples are in C# with NUnit. But language or testing frameworks are subjective and there is no one size fits all. My main goal here is to capture these patterns so that you can use them in your own exploits of writing Unit Tests. . Example Scenario . While this may get refuted later, lets have an example scenario. Consider we have a code for which we want to write a Unit Test. This code takes a User Name and Password, and assuming they are valid, generates an access token for the user and stores it in a cookie. Here is some example code for the same: . public class SignInUser : ISiteActions { public SignInUser(ICookierHelper cookieHelper, ITokenGenerator tokenGenerator, IDataProvider userDataProvider, IPasswordHasher hasher) { this.cookieHelper = cookieHelper; this.tokenGenerator = tokenGenerator; this.userDataProvider = userDataProvider; this.passwordHasher = passwordHasher; } public bool Execute(string username, string password) { var user = this.userDataProvider.getUser(username); if (user == null) return false; if(!this.passwordHasher.DoesMatch(password, user.salt, user.hash)) return false; var accessToken = this.tokenGenerator.Encode(username, user.userId); this.cookieHelper.setCookie(Constants.CookieName, accessToken); } } . Again this is just to set some context. These patterns can be easily applied to any kind of code we are testing. . Arrange-Act-Assert . This is the de-facto, canonical version of the Unit Test. This basically describes the structure of a Unit Test. Unit Tests usually test a single piece of code (say a public class method) and check whether one effect (or output of that method) was correct. Depending upon the complexity of the code being tested, some setup might be necessary, before the code under test can be executed. This pattern of setting up proper conditions for a test, running the code under test and then verifying the result is called arrange-act-assert pattern. For example, say we want to test the Execute method for the above code with a valid user name but invalid password, here is how we might write it. . public class SignInUserTests : SiteTestFrameworkBase { [Test] public void ShouldFailFor_InvalidPassword() { // Arrange var userSignIn = new SignInUser(GetMock&lt;ICookieHelper&gt;(), GetMock&lt;ITokenGenerator&gt;(), GetMock&lt;IDataProvider&gt;(), GetMock&lt;IPasswordHasher&gt;()); var username = this.GetValidUserName(); var password = this.MakePasswordDirty(username); // Act var result = userSignIn.Execute(username, password); // Assert Assert.IsFalse(result); } } . In this over simplified code, we are using Mock Dependencies to create an instance of the SignInUser class. We are also getting a valid username and getting an invalid password for that user. This all setup is part of Arrange. After that, we test the method in our Act section by passing our inputs and then validate the output in Assert section. . This code is an example of a strong Arrange-Act-Assert Pattern. This pattern is great when a specific situation is being tested or a set of specific inputs are tested. It is also great when you want to verify a single thing about the execution of your code. . But what happens when there are multiple side-effects of your code, your code doing multiple things? . One Act, Many Assertions . Sometimes, while testing some code, the code might have multiple side effects and we might want to make multiple assertions about these multiples effects that the code had. This seems simple, instead of having one assert, we can have multiple asserts in the end of our test. Many people are opposed to this, and while not being a strong problem, it might become a smell in your code. . Why are multiple assertions a smell? . Consider the case where your test code has multiple assertions in the end. One of the problems with this is that when a test fails, we might need to determine what part of code has failed. In this case, we might need to resort to going through stack traces to find out what actually failed. . Also, in NUnit (and a lot of other testing frameworks) the failure of a single assertion might cause the test execution to stop, so an early failure in this test can mask probable future failures in the test. . There is a solution to this problem, by using a method called specification testing or behavior driven development testing. Specification based testing techniques uses the specification of the program as the point of reference for test data selection. . For example, for the above code, we might want to test when a user successfully logs in, that result comes out true and the respective cookie is set. Consider the following code: . public class SignInUserTests : SiteTestFrameworkBase { [SetUp] public void SetUp() { var userSignIn = new SignInUser(GetMock&lt;ICookieHelper&gt;(), GetMock&lt;ITokenGenerator&gt;(), GetMock&lt;IDataProvider&gt;(), GetMock&lt;IPasswordHasher&gt;()); var username = this.GetValidUserName(); var password = this.GetValidPassword(username); var result = userSignIn.Execute(username, password); } [Test] public void ShouldSucceed() =&gt; Assert.IsTrue(result); [Test] public void ShouldSetCookie() =&gt; GetMock&lt;ICookieHelper&gt;().VerifyCalled(x =&gt; x.SetCookie(Constants.CookieName, TestConstants.TokenValue)); } . Since the above two results can fail indepedently from each other (we might not be able to set the cookie for some reason, for example), so putting both these assertions in the same test doesn‚Äôt make sense. . This type of test is great when your piece of code has multiple effects that can succeed or fail independently. . Test Cases . I like to call this pattern, Multiple Arranges, to match with the above title, but that‚Äôs probably wrong. This type of test is useful when we want to test a lot of different inputs matched with a lot of different outputs. Here we can do away with our example since it won‚Äôt be really helpful for this type of test. . Assume, we wrote an awesome function ConvertToEmoji that can convert the text description of an emoji to the actual emoji and we want to validate lots of different inputs and validate their outputs. We could write an Arrange-Act-Assert pattern type test for each input but that would have a lot of duplicate code and we want to maintain our test code maintainable. . NUnit has a great feature to write specifically these type of tests that can scale well (stay tuned after the example to know what to do if your test framework doesn‚Äôt have this feature). . This is how this type of test can be written in NUnit. . public class EmojiConverterTests { [TestCase(&quot;smile&quot;, &quot;üòÄ&quot;)] [TestCase(&quot;poop&quot;, &quot;üí©&quot;)] [TestCase(&quot;cry&quot;, &quot;üò™&quot;)] [TestCase(&quot;diamond&quot;, &quot;üíé&quot;)] [TestCase(&quot;ballon&quot;, &quot;üéà&quot;)] [TestCase(&quot;thumbs up&quot;, &quot;üëç&quot;)] [TestCase(&quot;pizza&quot;, &quot;üçï&quot;)] [TestCase(&quot;fire&quot;, &quot;üî•&quot;)] public void ConversionTests(string description, string emoji) { Assert.AreEqual(EmojiConverter.ConvertToEmoji(description), emoji) } } . What if your test library doesn‚Äôt support this? . Say your framework doesn‚Äôt support this type of feature, what can you do? . One way is that maybe you can have some sort of collection to store your input-output pairs and then iterate over the collection. It‚Äôs not quite as slick versus when this is built in the test library but it can still let you remove duplicate code and more importantly, let you add new tests easily. Example Code: . public void ConversionTests() { List&lt;string&gt; inputs = this.GetEmojiInputs(); List&lt;string&gt; outputs = this.GetEmojiOutputs(); for(int i = 0;i&lt;inputs.Count;i++) { Assert.AreEqual(EmojiConverter.ConvertToEmoji(inputs[i]), outputs[i]); } } . These type of tests are quite common for conversion of client contracts of an API to your server models or your server models to your database entity models (having separate models is actually quite a good pattern), where in we test with a valid set of inputs and invalid set of inputs (their number usually depends on the complexity of the objects being converted). Sometimes, we might not be able to pass in values of the test cases directly, then we can probably read the objects from a file system as well. . For example: . public class ClientServerModelsConverterTests { [TestCase(&quot;/inputs/valid1&quot;, &quot;/outputs/valid1&quot;)] [TestCase(&quot;/inputs/valid2&quot;, &quot;/outputs/valid2&quot;)] public void ConversionTests(string inputPath, string outputPath) { var inputObject = ReadFromFilePath&lt;ClientObj&gt;(inputPath); var outputObject = ReadFromFilePath&lt;ServerObj&gt;(outputPath); Assert.IsTrue(ObjectsAreEqual(inputObject.ToServerObject(), outputObject)); } } . (I know this code might have other issues, like how we can convert two inputs to one, but hey if you are nitpicking into my code, that means you read and understood it and mission accomplished üòé) . This pattern also makes it easy to add new test cases. And using NUnit‚Äôs pattern we can easily show that we are testing the conversion process, not just the conversion for one specific input. This lets you explain your intent in test cases quite clearly and that is quite helpful. . What type of test you should use? . As I said, there is no one size fits all and each type of unit test is great in its own context. And when in doubt, the standard arrange-act-assert is a great start! . Also, remember that test is code too. Like production code that will be deployed, we should make testing code more maintainable, avoid duplication and scalable to new cases. . I hope this post and the previous post gives you some ideas to help your test code be even better that it already is. üòÄ . Hope you enjoyed this post. If you just discovered my blog, do try reading some other posts from me. You can always connect with me on my socials to discuss or engage in conversations about software development with me. .",
            "url": "https://mitesh1612.github.io/blog/2020/11/27/different-types-UTs",
            "relUrl": "/2020/11/27/different-types-UTs",
            "date": " ‚Ä¢ Nov 27, 2020"
        }
        
    
  
    
        ,"post8": {
            "title": "A Primer on Different Kinds of Tests",
            "content": "I haven‚Äôt been a developer for too long (yet) to justify or even explain the importance of well written tests, especially in your CI system, but over my experience in recent times, I have seen great tests helping me catch a lot of future headaches. That being said, while developing a service end-to-end, I got to see the different kinds of tests and testing strategies and I thought why not ‚Äúblog my journey‚Äù on testing. So buckle up for a fun ride, into the land of testing. . The Importance of Testing . If you are working on a decent codebase, chances are that the codebase has its own CI System, whether using GitHub Actions, Azure DevOps, Travis CI, Circle CI, Jenkins. One of the most important tasks for a CI System is to run tests on your codebase, preferably so that merging your PR doesn‚Äôt break anything. . This is easier said, than done though. Having slow, long running tests in your Pull Request CI Builds slows down the check-in speed for a developer and makes you wait longer to get your changes merged. On the other hand, removing longer tests for something like Continuous Deployment builds puts you in the hazard of losing reliability and the quality signal of the master (or ‚Äúmain‚Äù) branches. Then it becomes a blame game of finding what broke the test and getting it fixed. Surely there must be a better way. . The different kinds of Tests . One of the main concerns for a test suite is the confusion around what each test should cover. Your tests might become too broad when they are expected to be focused or too focused when expected to have a broader testing. Thus, it becomes important to think about the kind of test you are writing while writing it. There are various ways to categorize tests. The famous types of tests are: . Unit Tests | Integration Tests | Acceptance Tests | . Unit Tests . These are small, micro tests, that test a single class or method, in isolation from its dependencies. This part is important. When you isolate the code under test, the tests become more focused and expressive. A test that touches any external resource such as the database (even if it is mock), file system, web service etc. is not a Unit Test. (Or might be? Stay tuned) . The purpose of Unit Tests is to validate that the piece of code under test is functionally correct and giving the proper expected output for a given input. Good Unit Tests also help by providing a documentation on how the class is expected to function for any consumers. . What are good unit tests? . Good Unit Tests are fast, atomic, isolated, conclusive and generally order independent. Usually they are executed automatically by the CI server on each commit to the version control system. . You should be careful about categorizing tests as Unit Tests. Like Code Smells, there are a lot of smells for Unit Tests like too much setup, long test methods, race conditions, lack of CI Integration. If your Unit Tests have these smells, consider a good refactor styled bath for your codebase. . There are a lot of test frameworks for a lot of different languages, like NUnit for C#, pytest for Python etc. which I don‚Äôt plan to cover here, they in general possess pretty good documentation. Although, stay tuned for another post on the various kinds of Unit Tests. . Integration Tests . You use them when your test requires some kind of an external dependency. So these tests includes testing the code that interacts with some external components like a data access layer, web service, file system etc. These tests provide real value when they use the actual real dependency (and not something mock). But due to this, they are harder to setup and slower to write and run than unit tests. That being said, these tests are absolutely necessary to have some kind of confidence in your test suite. . The primary purpose of these tests is to validate the code that work or manipulates an external system, but they also sometimes validate a portion of the remote system. For example if you use a repository pattern for data access and have a test that performs a Save() on a database, this test also in part checks the database connection, database engine, the network connection (if not onprem) etc. These kind of tests exercise a large block of code and infrastructure than unit tests, but that makes more brittle. . What are good integration tests? . Good Integration Tests validate the features of an external system that is used in your application. They do not attempt to cover the full set of functionality. Like Unit Tests, ideally these tests should be atomic, isolated, conclusive and order independent. . Smells for Integration Tests also include too much setup, long test methods, race conditions, lack of continuous integration. . Acceptance Tests . These are the tests that execute your entire stack, maybe not the user interface. Thus there is no usage of any kind of test doubles (mock objects/systems). The primary purpose of these tests is validate things like component wire up, application stack integration, basic use cases, system performance and stability. These tests are usually run the least often as they are time consuming to execute and require extensive setup (like deploying your services). . What are good acceptance tests? . Good acceptance tests can be understood by a user and are written in terms common to business. Their code smells include attempting to validate every path through the system. . There are also the category of UI Tests, but to be honest, I‚Äôm not really experienced much in those to write something meaningful about them, so I‚Äôll be skipping those. . Towards L0,L1,L2 Tests . Until now, all that I wrote seems kindo bookish, something you‚Äôd read in a Software Engineering book. So let me take you on a real tour of how testing is approached. In this wonderful article about DevOps in Microsoft, these guys explain how breaking tests into L0,L1,L2 etc helps them simplify and drastically improve their testing approach. . To hillariously simplify and summarize this article, they divide their tests as L0 and L1 which are still your unit tests and L2 and L3 which are functional/integration/acceptance tests. . Usually, they favour tests with fewest external dependencies and run majority of tests as part of the build process for a commit. If tests aren‚Äôt dependent, we could also run them in parellel, which gives faster CI build times. Although such L0 tests cannot test every aspect of the service, but the main point to not write a (functional) L2/L3 test where a (unit) L0 test could give the same information. . Since L2 and L3 tests are functional tests, they should always work with the public API of the product. . When we shift our focus on more L0 tests than L2/L3 tests, we make design implementations that support testability. . Shift-Left . . This kind of shift left approach lets you finish most of the testing before a change is merged into the master. . The new taxonomy of testing . In this way, tests were divided into the following categories: . L0 Tests: These are the fast, in-memory unit tests, the basic idea of a Unit test to most people as well. These tests depend on the code in the assembly and nothing else. These can be testing your business logic with assertions where given input returns an expected output. | L1 Tests: These are also Unit Tests but might require a Database or File System along with the assembly (can be mock). These dont include deploying the service. The most common tests here are for controller methods. For example say each route of your controller maps to a function, then you test that function (not deploy the controller) in a L1 test. | . # The controller route @route(/:userId) def getUser(userId): return userDb.Find(userId) # The L1 Test # Can set the db context to a mock db def test_getUser(): assert(getUser(1) == user[1]) . (This might not look like an actual test for python code, but I wanted to represent how to test controllers without deploying the service and then calling the service api path) . L2 Tests: These are Functional tests that run against ‚Äútestable‚Äù service deployment. These require deploying the service but you might mock some service dependencies for ease. For example, deploying the service in a test environment and hitting the public API to perform basic use cases | L3 Tests: These tests run against production and need a full product deployment. | . There are some quite good guidelines in the article on how to write good L0 and L2 tests that I strongly insist you to read (Unit Test Characteristics and Making Functional Tests Independent), since I don‚Äôt want to reiterate those same things. . Improving your Test Game . While I did provide 2 different taxonomies for categorizing tests, these are not the only way to categorize them. I tried to cover the various famous categories and share the example of L0, L1, L2 and L3 tests that was tried and tested (üòú) by me. While I might not be super knowledgeable in all the kinds of tests, you can share your experiences and techniques you use to improve your test suites, I‚Äôm open to discussion! üòÅ . If you liked this article, try reading some other articles on this blog. You can always follow me on my socials to connect, discuss with me. .",
            "url": "https://mitesh1612.github.io/blog/2020/10/06/software-testing",
            "relUrl": "/2020/10/06/software-testing",
            "date": " ‚Ä¢ Oct 6, 2020"
        }
        
    
  
    
        ,"post9": {
            "title": "The Naivete of Naive Bayes",
            "content": "So I finally figured out how to render LaTeX equations on my blog, and thus I can use my truly ‚Äúuseful‚Äù LaTex knowledge to start a new series on something that truly interests me. Machine Learning and how it works using the relative math in a simpler (and hopefully fun) language, to not scare away people who don‚Äôt prefer maths. Welcome to my series Machine Learning, with the Maths! (It sounds way cooler in my head!) . The point of this series is not to be a substitute for the plethora of tutorials already available on the topic of Machine Learning. I will try to explain the basic concepts behind each machine learning algorithm, and try to implement it from scratch using Python and NumPy. Maybe you can and hopefully will find better resources, but I hope this can be a good start for you. Plus, where else can I use my superbly huge and comically large Latex equations. Let‚Äôs begin, this probably very long post. . I am planning to write an introductory post on Bayes theorem anyways, so I am gonna skip the gory details here and link it in this post when I do end up writing it. Here‚Äôs the short intro to Bayes theorem. . There are a lot of ways to define what Bayes theorem represents. It can be thought of way to ‚Äúreverse‚Äù conditional probabilities. The interpretation we will be going through is that it can be used to find the probability of an event occurring, given the probability of another event that has already occurred. We can use (fun?) letters to denote some events, but hey, this is a Machine Learning post, so lets talk like that. Say you have a hypothesis (or an assumption) and you want to find the probability of that assumption, given the data. That is actually what Bayes theorem gives! Don‚Äôt believe me?. Let‚Äôs look at the equations . . This says that the gives the probability of event E occuring, given some event F occurs. Say event E represents our hypothesis and F represents the probability of obsering the data we observe. Then P(E/F) basically represents how much the data supports our assumption. Hence many times, Bayes theorem is also written like this . . The term P(Hypothesis) is also called prior or prior beliefs. Why prior? It is the degree of the conviction that the hypothesis is true before we observe the actual data. Remember, prior is the sometimes the trickiest terms to determine (as mentioned later here). The term P(Hypothesis/Data) is called the posterior probability, which represents ‚Äúthe possibility of the hypothesis, given the data‚Äù. Posterior is usually build after seeing the data. The term P(Data/Hypothesis) is a fun one. It represents the probability of having obtained the data, given the hypothesis and is called the likelihood term. In that way, P(Data) is the Evidence we have, or the data we have. . The evidence term can be broken as . P(F)=P(F‚à£E)P(E)+P(F‚à£E‚Ä≤)P(E‚Ä≤)P(F) = P(F|E)P(E) + P(F|E&amp;#x27;)P(E&amp;#x27;)P(F)=P(F‚à£E)P(E)+P(F‚à£E‚Ä≤)P(E‚Ä≤) . as well, which might come in handy later. . How Bayes Theorem is relevant to Machine Learning? . What Bayes theorem gives us is a framework to update our beliefs using evidence. Say you have a prior belief that some event occurs. Then you receive some evidence. You weigh your prior using the likelihood of that evidence happening and get a new belief, the posterior! Now when you get some more new evidence, you replace your prior belief with the posterior found about the event, and get a new posterior belief. And hey if you are not a probability nerd, let me explain this to you in a simpler way. Say you are a man who have lived his whole life inside a cave, and never got out. Now once you get out and see the sun rising on the east, what do you believe? Does it always happen or is this a one off? You assign some ‚Äúprior‚Äù probability of the sun rising in the east. The next day, the sun rises in east again and you update your belief using this evidence. Hmm, sun might rise on east on Mondays and Tuesdays and again update your belief (posterior). The more evidence you gather, the more you are sure about an event, and frankly that‚Äôs the crux of Machine Learning as well. We use data, or evidence, to update our beliefs about something. . Now with that out of the way, let‚Äôs turn to the example and actually implement this. . Spam me not! . The most common use of Naive Bayes, is for spam filters. Let‚Äôs look into how we can implement a spam filter using Naive Bayes from scratch. . Sometime ago, there was a scam on twitter where accounts of famous people like Elon Musk, Jeff Bezos etc. were hacked for ‚Äúbitcoin‚Äù scams, and let‚Äôs try to build a spam filter to help the poor souls who actually sent bitcoins to this scam. Let S be the event that the message is spam and B be the event that the message contains the word ‚Äòbitcoin‚Äô. Baye‚Äôs theorem tells us that the conditional probability that the message is spam, given it contains the word bitcoin is: . . The numerator is the probability that a message is spam and contains bitcoin, while the denominator is just the probability that a message contains bitcoin. How? P(B/S)P(S) = P(B,S) using the definition of conditional probablity (on which I did write a fun post you can read here) and the denominator is essentially P(B). In this sense, we can think of this calculation simply representing the proportion of bitcoin messages are spam. . Say we have a large number of messages that we know are spam and a large collection of messages that we know are ham (the word used for not spam commonly). Using that we can easily estimate P(B/S) and P(B/S‚Äô). Let‚Äôs make an assumption that it is equally likely that a message is spam or ham. Then P(S) = P(S‚Äô) = 0.5. Taking out the common term in the above equation, we get . . Now say we find out that 50% of the spam messages contain the word bitcoin but only 1% of non spam messages do, then the probablity that any given bitcoin-containing message is spam is: . 0.5/(0.5 + 0.01) = 98% . Making the Spam Filter more Sophisticated . Let‚Äôs bring in equations now (this aint called Machine Learning with Maths for no reason). Imagine we have a vocabulary of many words w1, w2, ‚Ä¶ ,wn and we say event Xi means the message contains the word wi. Also, since we are imagining so much, imagine that we have some undisclosed process to get an estimate of P(Xi/S) and P(Xi/S‚Äô), or basically the probability that a spam/not spam message contains the ith word. . Why is this Bayes Naive? . The key to Naive Bayes is making the (big) assumption that the presences (or absences) of each word are independent of one another, conditional on a message being spam or not. Intuitively, this assumption means that knowing whether a certain spam message contains the word bitcoin gives you no information about whether that same message contains the word rolex. In terms of ML terminologies, it makes the assumption that features of a measurement are independent of each other. . In math terms, this assumption means that: . . Now I am not going to lie, this is an extreme assumption, and the reason why it is called Naive Bayes. Say our vocabulary contains only the words ‚Äúbitcoin‚Äù and ‚Äúgold‚Äù and that half the messages that are spam are for ‚Äúearn bitcoin‚Äù and the other half messages that are spam are for ‚Äú26kt gold‚Äù. Thus P(Bitcoin/Spam) = 0.5 and P(Gold/Scam) = 0.5 as well. The probability that a message is spam which contains both ‚Äúbitcoin‚Äù and ‚Äúgold‚Äù is P(Bitcoin, Gold/Spam) = P(Bitcoin/Spam) * P(Gold/Spam) = 0.5 x 0.5 = 0.25. This happens since we assumed away the knowledge that ‚Äúbitcoin‚Äù and ‚Äúgold‚Äù never occur together. . The fun part is that, despite the unrealisticness of this assumption, this model often performs well and has historically been used in actual spam filters. . Using the same equation that we used for the bitcoin only spam filter, we can calculate the probability of a message being spam using the following equation: . . The Naive Bayes assumption allows us to compute each of the probabilities on the right simply by multiplying together the individual probability estimates for each vocabulary word and hence simplifies our calculation. . A practical consideration . In order to prevent underflow, where in computers don‚Äôt do well with floating-point numbers that are too close to 0, we try to avoid multiplying probability values. Instead of that, we can use log to multiply probabilities using log(ab) = log(a) + log(b) and then do an exp(logx) = x to get back the actual probability. This doesn‚Äôt change any of the equations or assumptions, it is just a small practical trick to avoid getting weird answers. . Training? . Now the only problem is estimating P(Xi/S) and P(Xi/S‚Äô), the probabilities that a spam message (or nonspam message) contains the word wi. If we have a fair number of ‚Äútraining‚Äù messages labeled as spam and not spam, an obvious first try is to estimate P(Xi/S) simply as the fraction of spam messages containing the word wi. . Being Smooth . While this calculation seems reasonable, it has a huge problem. Say, in our training messages, the word ‚Äúdata‚Äù only occurs in nonspam messages. Then we‚Äôd estimate P(data/S)=0. The result is that our Naive Bayes classifier would always assign spam probability 0 to any message containing the word data, even a message like ‚Äúdata on free bitcoin and 26 kt gold free.‚Äù To avoid this, we usually use some kind of smoothing. One of the simplest ways is to choose a pseudo count k (basically assuming that there are atleast k spam/ham messages containing the given word i). This gives us the following equation for estimating the probability of seeing the ith word in a spam or ham message as follows: . . We can do similarly for P(Xi/S‚Äô) where in we assume we also saw k additional nonspams containing the word and k additional nonspams not containing the word. . For example, if data occurs in 0/98 spam messages, and if k is 1, we estimate P(data/S) as 1/100 = 0.01, which allows our classifier to still assign some nonzero spam probability to messages that contain the word data. . Code Implementation . Now that we have all the pieces to build our classifier, the only thing to do is to actually build our classifier. This post contains code snippets but you can find the code on my GitHub repo here . Since we are planning to deal with text data with this classifier, we will be needing to tokenize our text to words/tokens. We assume we have a simple function tokenize() that returns all the tokens in a sentence. (A simple implementation for this would be to convert all text to lower case and then use regular expressions to remove special characters like apostrophes). We can obviously have complex tokenizing pipelines and more text pre-processings, but we will skip that for now. . Our training data consists of the message and a boolean is_spam indicating whether the message is spam?. We implement our classifier as a class to use it in a better way. The constructor only takes the parameter k. We also initialize a set to contain unique tokens, dictionaries which are counters to track how often each token is seen in spam messages and ham messages, and the counts of how many spam and ham messages it was trained on. . class NaiveBayesClassifier: def __init__(self, k = 0.5): self.k = k self.tokens = set() self.token_spam_counts = defaultdict(int) self.token_ham_counts = defaultdict(int) self.spam_messages = self.ham_messages = 0 . Now to implement the train function for it. According to the message type, we first increment the counts of spam or ham messages. Then we tokenize the message and then increment the spam/ham counts for each token. . def train(self, messages): for message in messages: if message.is_spam: self.spam_messages += 1 else: self.ham_messages += 1 # Increment word counts for token in tokenize(message.text): self.tokens.add(token) if message.is_spam: self.token_spam_counts[token] += 1 else: self.token_ham_counts[token] += 1 . Ultimately we‚Äôll want to predict P(spam / token). As we saw earlier, to apply Bayes‚Äôs theorem we need to know P(token / spam) and P(token / ham) for each token in the vocabulary. So we‚Äôll create a helper function to compute those. (_probabilities) . def _probabilities(self, token): &quot;&quot;&quot; Returns P(token/spam) and P(token/ham) &quot;&quot;&quot; spam = self.token_spam_counts[token] ham = self.token_ham_counts[token] p_token_spam = (spam + self.k) / (self.spam_messages + 2 * self.k) p_token_ham = (ham + self.k) / (self.ham_messages + 2 * self.k) return p_token_spam, p_token_ham . Finally we can write a predict method, and we will use the method of summing logs (as mentioned in here). This takes a message and tokenizes it. Then using the helper function it finds the probabilities of each token in the vocabulary. Then adds/multiplies the respective probability of seeing/not seeing the token in te message and then returns the probability of the given message being spam or ham. . def predict(self, text): text_tokens = tokenize(text) log_prob_if_spam = log_prob_if_ham = 0.0 # Iterate through each word in vocabulary for token in self.tokens: prob_if_spam, prob_if_ham = self._probabilities(token) # If token appears in message # add the log probability of seeing it if token in text_tokens: log_prob_if_spam += math.log(prob_if_spam) log_prob_if_ham += math.log(prob_if_ham) # Otherwise add the log probability # of not seeing it which is # log(1-probability of seeing it) else: log_prob_if_spam += math.log(1-prob_if_spam) log_prob_if_ham += math.log(1-prob_if_ham) prob_if_spam = math.exp(log_prob_if_spam) prob_if_ham = math.exp(log_prob_if_ham) . Inspecting the Model . We can even have a helper function that ‚Äúinspect‚Äù the model‚Äôs innards see which words are indicative of spam/not spam. . def p_spam_given_token(token, model): prob_if_spam, prob_if_ham = model._probabilities(token) return prob_if_spam/(prob_if_spam + prob_if_ham) words = sorted(model.tokens, key=lambda t: p_spam_given_token(t,model)) print(&quot;Spammiest Words: &quot;, words[-10:]) print(&quot;Hammiest Words: &quot;, words[:10]) . Trying out the model on a dataset . We will run this model on the UCI ML SMS Spam Dataset which can be found here. For the detailed code, you can visit this notebook in my GitHub repo . The model returns ‚Äúclaim‚Äù ‚Äúprize‚Äù as words indicating the message is spam which is a good sign of this performing okay. . Possible Improvements . How could we get better performance? One obvious way would be to get more data to train on. There are a number of ways to improve the model as well. Here are some possibilities that you might try: . Our classifier takes into account every word that appears in the training set, even words that appear only once. Modify the classifier to accept an optional min_count threshold and ignore tokens that don‚Äôt appear at least that many times. | The tokenizer has no notion of similar words (e.g., cheap and cheapest). Modify the classifier to take an optional stemmer function that converts words to equivalence classes of words. | Although our features are all of the form ‚Äúmessage contains word wi,‚Äù there‚Äôs no reason why this has to be the case. In our implementation, we could add extra features like ‚Äúmessage contains a number‚Äù by creating phony tokens like contains:number and modifying the tokenizer to emit them when appropriate. | . The horrors of prior . After all that code, there is one small topic I would like to touch on, which is on estimating priors. . Prior is one of the trickiest terms to determine in the Bayes equation. As explained really nicely in this video by Veritasium, Bayes theorem tells us how to update our beliefs in light of new evidence. It cant tell us how to set our prior beliefs. So it is possible for someone to have a different prior belief than other, because they are subjective. Some people might be more certain about a prior belief that other people. That‚Äôs how bias can creep in. And we definitely need a world, and a model, with lesser bias. . Mathematically, instead of ‚Äúchoosing‚Äù a prior, we assume the prior probability to follow a prior model and we try to estimate these model parameters (for example assuming the prior distribution follows Gaussian distribution and finding its parameters) . For further exploration . You can visit this blog for another ‚Äúfrom scratch‚Äù implementation of Naive Bayes or this blog as well . . The idea and much of the code was from the book ‚ÄúData Science from Scratch‚Äù which is truly an amazing read for someone who wants to implement stuff from scratch. . If you stuck with me till this long, it seems you enjoyed this post. I hope to keep updating this new Machine Learning, with the Maths series so keep an eye on this blog. You can always @ me at my socials or the GitHub repo for this blog. Thanks for reading. üòä .",
            "url": "https://mitesh1612.github.io/blog/2020/08/30/naive-bayes",
            "relUrl": "/2020/08/30/naive-bayes",
            "date": " ‚Ä¢ Aug 30, 2020"
        }
        
    
  
    
        ,"post10": {
            "title": "C# Properties and their different Uses",
            "content": "I learnt C# in parts from various resources, mostly because each one had their different starting points. One book heavily relied on using Visual Studio which definitely makes life easy, but makes you ignore the nitties-gritties of setting up .NET projects, while one did everything from scratch, which did scratch my itch (Get it? üòú) but had a really really slow pace. . One constructs that I learnt about while seeing how to create classes was C# Properties. Properties have a little confusing syntax as described here (in all the non syntax highlighted glory of C#) . public class MyRiches { // Normal Data Members public int Money; // Properties public int GoldInKgs { get; set; } } . If you come from some other language, upon seeing this, you might be like . When I learnt about properties, I kept wondering why do I work extra hard to define data members when I can do it in a simpler and a much known way. I though it was just syntactic sugar, only with its visible diabetes as well. Then I saw an interesting usage of properties in our existing codebase, something like this . public class MyClass { public IList&lt;SomeType&gt; Property { get { if(m_property == null) { m_property = new List&lt;SomeType&gt;(); } return m_property; } set { m_property = value; } } private IList&lt;SomeType&gt; m_property; } . and I suddenly doubting my already dubious C# knowledge. Thus I began a journey on to understand the actual use and various usage of patterns. (Don‚Äôt worry, I will also explain this example soon) . So what actually are Properties? . Before getting into why these exist, lets assume they didnt. Now say I had a data member in my class that was private but I needed to access it‚Äôs value in some other class, but not let it modify the value. For example . public class Bank { private int AccountBalance; } public class Me { var myBankAccount = new Bank(); var myMoney = myBankAccount.AccountBalance; } . Now if I cant directly access the AccountBalance property. If I make it public, although it might be fun for me, it might not be fun for the bank, when I could just do this . // The easy way of becoming a millionaire myBankAccount.AccountBalance = 1000000; . So what is the other solution? Ah yes, getters (and setters, their siblings). We could easily define a getter on this data member that will get us the value but not let modify it. Something like this: . public class Bank { private int AccountBalance; public int getAccountBalance() { return AccountBalance; } } public class Me { var myBankAccount = new Bank(); var myMoney = myBankAccount.getAccountBalance(); } . If you are used to working in Java with Eclipse, you know it has a functionality of auto generating getters and setters and I‚Äôm partly sure the creators of C# might have already loaded this in Visual Studio, but for people who didnt use it, this was a long and tedious methods to write this getters and setters. That‚Äôs why, C# creators created properties. . Properties provide an access mechanism for private data members . So even though you can essentially create data members using properties, that‚Äôs not their intended use. Essentially Properties provide a flexible mechanism to read, write or compute the value of a private field. They can be used as data members, but they are actually special methods called accessors, which as you guessed it, are useful for accessing data. . If you are not a fan of big words, this basically means, they are a shortcut for writing customized getter and setter methods. Thus, in a way, they indeed are syntactic sugar, without the harms? I am not going to bore you with the syntax of C# Properties, here is a good reference. . Now comes the fun part, the various usage patterns of Properties. The get and set aren‚Äôt just for show. You can customize them to implement various fun and useful patterns in your code, and here I‚Äôll show a few . Lazy Loading Values using Properties . Properties can help you implement a cache with lazy loading feature. For example . private int m_IncomeTax; public int IncomeTax { get { if(m_IncomeTax == null) { m_IncomeTax = AReallyLongComputationForTax(); } return m_IncomeTax; } } . This is the other primary usage of Properties, of course other than controlling access. . Future Proofing Code . Say you want to maintain the API of your class but the logic or calculation changes. To incorporate that without affecting your class API, you can change the setter code. . In terms of the above example, say in the computation for tax, they include a cess (YES! Tax on Tax!), you can change the getters like this, such that IncomeTax property gives you the total tax always. . // Old public int IncomeTax { get { return m_IncomeTax; } } // New public int IncomeTax { get { return m_IncomeTax + Cess; } } . Creating a Contract . Properties help you create a contract/API for a class. This way you can have a proper contract of accessing class members, which might be private or calculations on some private members. They are useful if you need some extra calculations on private members. . In general, the point is to separate implementation (the field) from API (the property). Later on you can, should you wish, put logic, logging etc in the property without breaking either source or binary compatibility - but more importantly you‚Äôre saying what your type is willing to do, rather than how it‚Äôs going to do it. More on this in this article . Returning Non Null Values using Properties . I promised I‚Äôll explain the code at the beginning of the section. For reference, this is the code . public class MyClass { public IList&lt;SomeType&gt; Property { get { if(m_property == null) { m_property = new List&lt;SomeType&gt;(); } return m_property; } set { m_property = value; } } private IList&lt;SomeType&gt; m_property; } . This code essentially checks whether a given member is null, and if it is null, it will populate the value first, and then return it. There is one great benefit of using this approach. Anything that consumes the value of this property need not have a null check, since this essentially ensures that you never get a null value and reduces the amount of code and checks you need to write, and don‚Äôt we all want to write less code? . In closing, I know properties seem like glorified setters/getters and all of the benefits mentioned above can also be done using setters and getters as well, as said above, they are just syntactic sugar. Learning how they can be used to control access rather than being used as public data members can help flesh out some nice code as well. . An honourable TypeScript Mention . Typescript itself has a similar method of implementing getters on private variables, which I remembered when I was reading up on properties. This looks quite similar to the C# Properties implementation and hence the mention here. . class MyClass { private _property; public get property() { return _property; } } var data = new MyClass(); var value = data.property; .",
            "url": "https://mitesh1612.github.io/blog/2020/08/19/c-sharp-properties",
            "relUrl": "/2020/08/19/c-sharp-properties",
            "date": " ‚Ä¢ Aug 19, 2020"
        }
        
    
  
    
        ,"post11": {
            "title": "Conditional Probability and Families",
            "content": "So I was reading the book ‚ÄúData Science from Scratch‚Äù by Joel Grus. I came across this nice tricky problem in Conditional Probability mentioned in this book. Let‚Äôs take a look . A Quick Refresher on Conditional Probability . Well if you forgot the definition of conditional probability, here is a small refresher for you. Say there are two events E and F. The probability that E happens given that we know F happens is represented using . P(E‚à£F)P(E|F)P(E‚à£F) . Mathematically, . P(E‚à£F)=P(E,F)/P(F).P(E,F)P(E|F) = P(E,F)/P(F). P(E,F)P(E‚à£F)=P(E,F)/P(F).P(E,F) . is the probability of both E and F happening. . Well enough of a probability class, lets look at an interesting family now . The Family and The Unknown Children . Say there is a family with two unknown children. If we assume that: . Each child is equally likely to be a boy or a girl | The gender of the second child is independent of the first child. | . Then the event ‚Äúno girls‚Äù has probability 1/4, the event ‚Äúone girl, one boy‚Äù has probability 1/2, and the event ‚Äútwo girls‚Äù has probability 1/4. . Now here comes the interesting problem. ‚ÄúWhat is the probability of the event ‚Äòboth children are girls‚Äô (B) conditional on the event that ‚Äòthe older child is a girl (G)‚Äô?‚Äù . This aint a test, so here is how we can easily calculate this using conditional probability . P(B‚à£G)=P(B,G)/P(G)P(B|G) = P(B,G)/P(G)P(B‚à£G)=P(B,G)/P(G) . T event B and G (‚Äúboth children are girls and the older child is a girl‚Äù) is just the event B. (Once you know that both children are girls, it‚Äôs necessarily true that the older child is a girl). Thus, . P(B‚à£G)=P(B,G)/P(G)=P(B)/P(G)=1/2P(B|G) = P(B,G)/P(G) = P(B)/P(G) = 1/2P(B‚à£G)=P(B,G)/P(G)=P(B)/P(G)=1/2 . This is mostly intuitive. Now can you guess ‚ÄúWhat is the probability of the event ‚Äòboth children are girls‚Äô (B) conditional on the event that ‚Äòat least one of the children is a girl‚Äô (L)?‚Äù . Surprisingly, the answer to this question is different from the one before. Here is how . As before, the event B and L (‚Äúboth children are girls and at least one of the children is a girl‚Äù) is just the event B. This means we have: . P(B‚à£L)=P(B,L)/P(L)=P(B)/P(L)=1/3P(B|L) = P(B,L)/P(L) = P(B)/P(L) = 1/3P(B‚à£L)=P(B,L)/P(L)=P(B)/P(L)=1/3 . How can this be the case? Well, if all you know is that at least one of the children is a girl, then it is twice as likely that the family has one boy and one girl than that it has both girls. . Still don‚Äôt believe me? . Well if you are the doubting kind, we can write a simple Python code to simulate this experiment. . import enum, random class Kid(enum.Enum): BOY = 0 GIRL = 1 def random_kid() -&gt; Kid: return random.choice([Kid.BOY, Kid.GIRL]) both_girls = 0 older_girl = 0 either_girl = 0 random.seed(20200814) # Yep, this post&#39;s date for _ in range(10000): younger = random_kid() older = random_kid() if older == Kid.GIRL: older_girl += 1 if older == Kid.GIRL and younger == Kid.GIRL: both_girls += 1 if older == Kid.GIRL or younger == Kid.GIRL: either_girl += 1 print(&quot;P(both | older):&quot;, both_girls / older_girl) # 0.494 ~ 1/2 print(&quot;P(both | either): &quot;, both_girls / either_girl) # 0.322 ~ 1/3 . This indicates how the conditioning of an event affects its probability. . Weirdly, this problem reminds me a lot of the Monty Hall Problem. Hey if you have any explaination on how this can relate to Monty Hall, you can always @ me at my socials (linked in this blog at various locations!üòâ) and I‚Äôll probably try to figure out if there is any relation between these two (after I refresh my Probability Course üòã) .",
            "url": "https://mitesh1612.github.io/blog/2020/08/14/conditional-probability",
            "relUrl": "/2020/08/14/conditional-probability",
            "date": " ‚Ä¢ Aug 14, 2020"
        }
        
    
  
    
        ,"post12": {
            "title": "Constructing a complex dictionary of base and derived class using the same code",
            "content": "Okay, really complex title aside, I‚Äôll try to explain the problem I had and the interesting way I found to solve it. This may not be the best or the most optimized way, but I really liked this solution and would like to share it. . Also, although I do prefer Python, this post is in C#, since that was the language I encountered this problem in. . So here is the problem. Say we have two C# Classes . public class BaseClass { public string SomeProperties { get; set; } } public class DerivedClass : BaseClass { public string SomeOtherProperties { get; set; } } . I wanted to construct a Dictionary of these classes like Dictionary&lt;string, BaseClass&gt; and Dictionary&lt;string, DerivedClass&gt; at two very different places. The construction of each dictionary element was not that trivial due to the inherent complexity of filling the properties for both the classes. Here is an example of how one of the dictionaries was being created: . // For the base class var map = new Dictionary&lt;string, BaseClass&gt;(); foreach(var someProperty in someList) { var baseElement = new BaseClass(someProperty); map[someProperty] = baseElement; } foreach(var dependency in dependencyList) { map[dependency.To] = map[dependency.From] } // For the derived class var map = new Dictionary&lt;string, DerivedClass&gt;(); foreach(var someProperty in someList) { // someOtherProperty comes from somewhere else var derivedElement = new DerivedClass(someProperty, someOtherProperty); map[someProperty] = derivedElement; } foreach(var dependency in dependencyList) { map[dependency.To] = map[dependency.From] } . As you can see, there is a lot of logic repeating since the properties that were created for the base class were also created for the derived class, but when we create the DerivedClass object, new properties were also to be added to those object. Both the objects differ in how they are constructed but the way the map is created is similar. I wanted a way to reuse these for loops instead of writing them for both BaseClass and DerivedClass and other classes that might inherit from BaseClass later. . My basic idea was to use a Template method like this. . public Dictionary&lt;string, &lt;T&gt;&gt; CreateDictionary(parameters) { while(someConditionOnParameters) { if(T is BaseClass) { // Base class object creation code } else if(T is DerivedClass) { // Derived class object creation code } // repeated dictionary creation code } } . The other problem that I encounter here in was the arguments to this method. When we create the BaseClass object, I require fewer properties but when I created DerivedClass object, I require more properties and hence the number and type of arguments couldn‚Äôt be fixed. Of course, I can set/pass them as null and ignore when not needed, but that didn‚Äôt feel like a tidy solution to me. Plus later on, when we derive a new class from BaseClass, again the signature of method changes which might break a few things here and there. . That‚Äôs when I was suggested the interesting solution to this problem, the one I am going to share now. We keep one function that creates this dictionary but rather than passing the parameters to create the objects, we pass a function that creates those objects for us. For example when we want to create the BaseClass dictionary, we can pass a function that creates the base class object and so on. This way this method can be extensible for any classes that derive from future as well. Here is a dummy code to show how that method might look like . public Dictionary&lt;string, BaseClass&gt; CreateDictionary(DataObject requiredData, Func&lt;Data, BaseClass&gt; objectCreator) { var map = new Dictionary&lt;string, BaseClass&gt;(); foreach(Data propertyValues in requiredData.data) { var element = objectCreator(Data); } foreach(var dependency in requiredData.dependencies) { map[dependency.To] = dependency.From; } } . Now when I want to create the base class dictionary, I can call it like: . var map = CreateDictionary(requiredData, x =&gt; { return new BaseClass(x.somePropertyValue); }); . Or if I want the derived class dictionary, like this: . var map = CreateDictionary(requiredData, x =&gt; { return new DerivedClass(x.SomePropertyValue, someOtherPropertyValue); }); . I really loved this solution, its nifty and useful and this didnt come to my mind easily. . Closing Thoughts . I know this is a really specific and weird problem to encounter, and some constraints of why this solution was used over other ways are not clear from the vague names and class designs (and possibly incomplete details) I provided. However, I really found the solution interesting and felt like sharing it. . You can always share your thoughts on this by @‚Äôing me on Twitter or LinkedIn (links are available in my author bio) or even this blog‚Äôs GitHub repo. .",
            "url": "https://mitesh1612.github.io/blog/2020/08/13/constructing-objects",
            "relUrl": "/2020/08/13/constructing-objects",
            "date": " ‚Ä¢ Aug 13, 2020"
        }
        
    
  
    
        ,"post13": {
            "title": "Welcome to this blog",
            "content": "Welcome to this blog . Hey everyone, welcome to my blog. I finally took the leap and set up my blog, and after a lot of thinking, I finally decided to create a blog. . Why a blog? Well among a lot of other reasons, like not forgetting what I learn from time to time, and to keep track of my journey as a Software Development Engineer. The other huge reason is that I love to learn new things, research the ones I find interesting, and I plan to document and share my learnings via blog posts, which can be easily found and referred to later as well. I‚Äôll try to share things I learn about Software Development, my interests in Data Science and all the other random things I encounter. Hopefully, other people will also find these posts helpful, relevant or interesting. You can always share your views on my GitHub repo here (until I figure out adding comment sections to a static GitHub site üòâ) . Thanks for visiting this blog! . About Me . I‚Äôm Mitesh Shah, and I live in Hyderabad. I started my journey as a Software Engineer at Microsoft (where I currently work). I have a lot of interest in Data Science and Machine Learning and I love reading books or playing some games in my relaxing time. . Technical Details for this Blog . This blog is possible due to Gatsby JS with the wonderful theme Novela created by the Narative Team. Right now this site is hosted on GitHub pages using a CI from GitHub Actions . Update : 24-12-2020 . Since then, I have moved this blog to Fastpages. The links in this post are updated now. .",
            "url": "https://mitesh1612.github.io/blog/2020/08/12/welcome-to-this-blog",
            "relUrl": "/2020/08/12/welcome-to-this-blog",
            "date": " ‚Ä¢ Aug 12, 2020"
        }
        
    
  

  
  

  
      ,"page1": {
          "title": "About Me",
          "content": ". Hey there. I am the writer, Mitesh Shah, a Software Developer from India, casual gamer, gadget hoarder and learning more Software Development. Right now I work as a Software Engineer in Microsoft and love to read up on various topics like Data Science, Machine Learning, Backend Development in my free time. . I have been professionally writing code since 2019 (until then it was mostly academic) and while on my job, I have worked on writing scalable systems for Microsoft Azure. I get to learn a lot of things, things that I feel like sharing, or at least storing them for my own references later. So expect posts related to software development, writing maintainable code, but also posts related to Data Science, Machine Learning as well. . Tech you can talk to me about: . Backend Development using Flask, Node + Express, ASP.NET | Making Databases using MongoDB, MySQL | Frontend Development using Bootstrap, React (with Flux/Redux) | Creating cloud applications using Azure | Programming languages like C++, C#, Python, JavaScript / TypeScript | Machine Learning and the Maths behind it | . I like Machine Learning, and love understanding the why behind everything. To facilitate that, I often try to implement the common Machine Learning algorithms from scratch. . Feel free to connect with me on my socials: . &nbsp;&nbsp; &nbsp;&nbsp; .",
          "url": "https://mitesh1612.github.io/blog/about/",
          "relUrl": "/about/",
          "date": ""
      }
      
  

  

  
  

  

  
  

  

  
  

  
  

  
  

  
      ,"page10": {
          "title": "",
          "content": "Sitemap: {{ ‚Äúsitemap.xml‚Äù | absolute_url }} | .",
          "url": "https://mitesh1612.github.io/blog/robots.txt",
          "relUrl": "/robots.txt",
          "date": ""
      }
      
  

}